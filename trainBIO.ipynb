{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from mydata.ipynb\n",
      "Train:4520 Test:503\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from mydata import mydata\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from transformers import *\n",
    "import pandas as pd\n",
    "import ast\n",
    "import copy\n",
    "import os\n",
    "from time import strftime,gmtime\n",
    "from opencc import OpenCC\n",
    "import pyprind\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Model\n",
    "class bertwwm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(bertwwm,self).__init__()\n",
    "        self.bert_model = BertModel.from_pretrained(\"hfl/chinese-bert-wwm\")\n",
    "        #self.tokenizer = BertTokenizer.from_pretrained(\"hfl/chinese-bert-wwm\")\n",
    "        self.bi_decoder = nn.Sequential(\n",
    "            nn.Linear(768,1)\n",
    "            ,nn.Dropout(0.1)\n",
    "            ,nn.Sigmoid()\n",
    "        )\n",
    "        self.start_Linear = nn.Sequential(\n",
    "            nn.Linear(768,512)\n",
    "            ,nn.Dropout(0.1)\n",
    "            ,nn.Sigmoid() \n",
    "        )\n",
    "        self.end_Linear = nn.Sequential(\n",
    "            nn.Linear(768,512)\n",
    "            ,nn.Dropout(0.1)\n",
    "            ,nn.Sigmoid() \n",
    "        )\n",
    "    def forward(self,input_ids):\n",
    "        #print(input_ids.shape)\n",
    "        # input a string\n",
    "        all_hidden_states, all_attentions = self.bert_model(input_ids)[-2:]\n",
    "        binary = self.bi_decoder(all_attentions).reshape(1)\n",
    "        s = self.start_Linear(all_attentions).reshape(512)\n",
    "        e = self.end_Linear(all_attentions).reshape(512)\n",
    "       # print(binary.shape,binary)\n",
    "        return binary,s,e\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class bertwwm_pos(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(bertwwm_pos,self).__init__()\n",
    "        self.bert_model = BertModel.from_pretrained(\"hfl/chinese-bert-wwm\")\n",
    "        #self.tokenizer = BertTokenizer.from_pretrained(\"hfl/chinese-bert-wwm\")\n",
    "        self.start_Linear = nn.Sequential(\n",
    "            nn.Linear(768,512)\n",
    "            ,nn.Dropout(0.1)\n",
    "            ,nn.Sigmoid() \n",
    "        )\n",
    "        self.end_Linear = nn.Sequential(\n",
    "            nn.Linear(768,512)\n",
    "            ,nn.Dropout(0.1)\n",
    "            ,nn.Sigmoid() \n",
    "        )\n",
    "    def forward(self,input_ids):\n",
    "        #print(input_ids.shape)\n",
    "        # input a string\n",
    "        all_hidden_states, all_attentions = self.bert_model(input_ids)[-2:]\n",
    "        #binary = self.bi_decoder(all_attentions).reshape(1)\n",
    "        s = self.start_Linear(all_attentions).reshape(512)\n",
    "        e = self.end_Linear(all_attentions).reshape(512)\n",
    "       # print(binary.shape,binary)\n",
    "        return s,e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef test(model,data,device):\\n    print(\\'Testing...\\')\\n    size,loss,acc,not_acc = 0,0,0,0\\n    test_num = len(data.test)\\n    criterion = nn.BCELoss()\\n    tokenizer = BertTokenizer.from_pretrained(\"hfl/chinese-bert-wwm\")\\n    model.eval()\\n    print(f\\'Testing data: {test_num}\\')\\n    with torch.no_grad():\\n        for content,name,s,e in data.test:  \\n            if len(name) is 0:    \\n                label = torch.Tensor([0]).to(\\'cpu\\')\\n            else:\\n                label = torch.Tensor([1]).to(\\'cpu\\')\\n            size +=1 \\n            input_ids = torch.tensor([tokenizer.encode(str(content),max_length=512,truncation=True)]).to(device)\\n            b_pred,s_pred,e_pred = model(input_ids).to(\\'cpu\\')\\n            a_loss = criterion(pred,label) \\n            loss += a_loss\\n            if len(name) is 0 and pred.item() < 0.4:\\n                #print(pred.item())\\n                acc +=1\\n            elif len(name) is not 0 and pred.item() >0.4:\\n                acc +=1    \\n    loss /= test_num\\n    return loss,acc\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def test(model,data,device):\n",
    "    print('Testing...')\n",
    "    size,loss,acc,not_acc = 0,0,0,0\n",
    "    test_num = len(data.test)\n",
    "    criterion = nn.BCELoss()\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"hfl/chinese-bert-wwm\")\n",
    "    model.eval()\n",
    "    print(f'Testing data: {test_num}')\n",
    "    with torch.no_grad():\n",
    "        for content,name,s,e in data.test:  \n",
    "            if len(name) is 0:    \n",
    "                label = torch.Tensor([0]).to('cpu')\n",
    "            else:\n",
    "                label = torch.Tensor([1]).to('cpu')\n",
    "            size +=1 \n",
    "            input_ids = torch.tensor([tokenizer.encode(str(content),max_length=512,truncation=True)]).to(device)\n",
    "            b_pred,s_pred,e_pred = model(input_ids).to('cpu')\n",
    "            a_loss = criterion(pred,label) \n",
    "            loss += a_loss\n",
    "            if len(name) is 0 and pred.item() < 0.4:\n",
    "                #print(pred.item())\n",
    "                acc +=1\n",
    "            elif len(name) is not 0 and pred.item() >0.4:\n",
    "                acc +=1    \n",
    "    loss /= test_num\n",
    "    return loss,acc\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,data,device):\n",
    "    print('Testing...')\n",
    "    size,loss,acc,not_acc = 0,0,0,0\n",
    "    test_num = len(data.test)\n",
    "    criterion = nn.BCELoss()\n",
    "    cross = nn.CrossEntropyLoss()\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"hfl/chinese-bert-wwm\")\n",
    "    model.eval()\n",
    "    print(f'Testing data: {test_num}')\n",
    "    B_loss,S_loss,E_loss = 0,0,0\n",
    "    with torch.no_grad():\n",
    "        for content,name,s,e in data.train:\n",
    "            i +=1 \n",
    "            input_ids = torch.tensor([tokenizer.encode(str(content),max_length=512,truncation=True)]).to(device)   \n",
    "            b_pred,s_pred,e_pred = model(input_ids)\n",
    "            b_pred = b_pred.to('cpu')\n",
    "            s_pred = s_pred.to('cpu')\n",
    "            e_pred = e_pred.to('cpu')\n",
    "        # Binary:\n",
    "            if len(name) is 0:    \n",
    "                b_label = torch.Tensor([0]).to('cpu')\n",
    "            else:\n",
    "                b_label = torch.Tensor([1]).to('cpu')\n",
    "            \n",
    "            if len(name) is 0 and pred.item() < 0.4:\n",
    "                acc +=1\n",
    "            elif len(name) is not 0 and pred.item() >0.4:\n",
    "                acc +=1 \n",
    "        # Start\n",
    "            s_label = [0]*512\n",
    "            for s_ in s:\n",
    "                s_label[s_] = 1\n",
    "            s_label = torch.Tensor(s_label).to('cpu')\n",
    "            print(s_label)\n",
    "        # End\n",
    "            e_label = [0]*512\n",
    "            for e_ in e:\n",
    "                e_label[e_] = 1\n",
    "            e_label = torch.Tensor(e_label).to('cpu')\n",
    "        # count loss\n",
    "            a_b_loss = criterion(b_pred,b_label)\n",
    "            a_s_loss = criterion(s_pred,s_label)\n",
    "            a_e_loss = criterion(e_pred,e_label)\n",
    "            B_loss += a_b_loss\n",
    "            S_loss += a_s_loss\n",
    "            E_loss += a_e_loss\n",
    "            \n",
    "    # Averge        \n",
    "    B_loss /= test_num\n",
    "    S_loss /= test_num\n",
    "    E_loss /= test_num\n",
    "    \n",
    "    return B_loss,S_loss,E_loss,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr  : learning rate\n",
    "# w_d :weight_decay\n",
    "\n",
    "def train(data,lr_rate ,w_d,device):\n",
    "    model = bertwwm_pos().to(device)\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = optim.Adadelta(parameters, lr=lr_rate, weight_decay=w_d)\n",
    "    criterion = nn.BCELoss()\n",
    "    cross = nn.CrossEntropyLoss()\n",
    "    #cc = OpenCC('tw2sp')\n",
    "    max_test_acc = 0\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"hfl/chinese-bert-wwm\")\n",
    "    i = 0\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    pbar = pyprind.ProgBar(len(data.train))\n",
    "    for content,name,s,e in data.train:\n",
    "        i +=1 \n",
    "        \n",
    "        input_ids = torch.tensor([tokenizer.encode(str(content),max_length=512,truncation=True)]).to(device)   \n",
    "        \n",
    "        #b_pred,s_pred,e_pred = model(input_ids)\n",
    "        \n",
    "        #b_pred = b_pred.to('cpu')\n",
    "        \n",
    "        s_pred,e_pred = model(input_ids)\n",
    "        del input_ids\n",
    "        \n",
    "        s_pred = s_pred.to('cpu')\n",
    "        e_pred = e_pred.to('cpu')\n",
    "               \n",
    "        for task in range(0,3):\n",
    "            # Binary:\n",
    "            if task is -1:\n",
    "                if len(name) is 0:    \n",
    "                    b_label = torch.Tensor([0]).to('cpu')\n",
    "                else:\n",
    "                    b_label = torch.Tensor([1]).to('cpu')\n",
    "                a_b_loss = criterion(b_pred,b_label)\n",
    "                a_b_loss.backward(retain_graph=True)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "             # Start\n",
    "            elif task is 1:\n",
    "                s_label = [0]*512\n",
    "                for s_ in s:\n",
    "                    s_label[s_] = 1\n",
    "                s_label = torch.Tensor(s_label).to('cpu')\n",
    "                a_s_loss = criterion(s_pred,s_label)\n",
    "                a_s_loss.backward(retain_graph=True)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            elif task is -1:\n",
    "            # End\n",
    "                e_label = [0]*512\n",
    "                for e_ in e:\n",
    "                    e_label[e_] = 1\n",
    "                e_label = torch.Tensor(e_label).to('cpu')\n",
    "                a_e_loss = criterion(e_pred,e_label)\n",
    "                a_e_loss.backward(retain_graph=True)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            #optimizer.step()\n",
    "            \n",
    "         # count loss \n",
    "        # backward    \n",
    "        #loss += (a_b_loss + a_s_loss + a_e_loss)/3\n",
    "        #print(f'Train loss {loss}') \n",
    "        #if (i+1) % 4 is 0:\n",
    "        \n",
    "            #print(f'Batch loss is {loss/8}')\n",
    "            #loss = 0\n",
    "        pbar.update()\n",
    "        \"\"\"\n",
    "        if (i+1) % 1000 is 0:\n",
    "            test_loss,test_acc = test(model,data,device)\n",
    "            print(f'Test Loss is {test_loss} ACC is {test_acc}')\n",
    "            if test_acc > max_test_acc:\n",
    "                print('Better model!')\n",
    "                best_model = copy.deepcopy(model.state_dict())\n",
    "        model.to(device)\n",
    "        model.train()\n",
    "        \"\"\"\n",
    "    best_model = copy.deepcopy(model.state_dict())\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:4520 Test:503\n",
      "Data load down\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "device = torch.device('cuda:0')\n",
    "data = mydata('./train_fix.csv')\n",
    "print('Data load down')\n",
    "mode = 'train'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:04:47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train end, model name is bertWWM_SE_15_37_51.pt\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for bertwwm:\n\tMissing key(s) in state_dict: \"bi_decoder.0.weight\", \"bi_decoder.0.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-324f98dcb056>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Train end, model name is {modelname}.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbertwwm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'saved_models/{modelname}.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtest_b_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_s_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_e_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'LOSS: B{test_b_loss} S{test_s_loss} E{test_e_loss} ACC is {test_acc}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 847\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    848\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for bertwwm:\n\tMissing key(s) in state_dict: \"bi_decoder.0.weight\", \"bi_decoder.0.bias\". "
     ]
    }
   ],
   "source": [
    "if mode is 'train':\n",
    "    best_model = train(data,0.005,5e-5,device)\n",
    "    \n",
    "    if not os.path.exists('saved_models'):\n",
    "        os.makedirs('saved_models')    \n",
    "    modeltime = strftime('%H_%M_%S', gmtime()) \n",
    "    modelname = 'bertWWM_SE_'+ modeltime\n",
    "    torch.save(best_model, f'saved_models/{modelname}.pt')\n",
    "    print(f'Train end, model name is {modelname}.pt')\n",
    "    test_model = bertwwm().to(device)\n",
    "    test_model.load_state_dict(torch.load(f'saved_models/{modelname}.pt'))\n",
    "    test_b_loss,test_s_loss,test_e_loss,test_acc = test(test_model,data,device)\n",
    "    print(f'LOSS: B{test_b_loss} S{test_s_loss} E{test_e_loss} ACC is {test_acc}')\n",
    "    \n",
    "elif mode is 'testb':\n",
    "    modelname = 'bertWWM_SE_15_37_51.pt'\n",
    "    test_model = bertwwm().to(device)\n",
    "    test_model.load_state_dict(torch.load(f'saved_models/{modelname}'))\n",
    "    test_b_loss,test_s_loss,test_e_loss,test_acc = test(test_model,data,device)\n",
    "    print(f'LOSS: B{test_b_loss} S{test_s_loss} E{test_e_loss} ACC is {test_acc}')\n",
    "\n",
    "elif mode is 'tests':\n",
    "    modelname = 'bertWWM_SE_15_37_51.pt'\n",
    "    test_model = bertwwm_pos().to(device)\n",
    "    test_model.load_state_dict(torch.load(f'saved_models/{modelname}'))\n",
    "    test_b_loss,test_s_loss,test_e_loss,test_acc = test(test_model,data,device)\n",
    "    print(f'LOSS: B{test_b_loss} S{test_s_loss} E{test_e_loss} ACC is {test_acc}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if __name__ == '__main__':\n",
    "#    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bertWWM_06_41_57.pt : 500/503\n",
    "# bertWWM_06_52_12.pt.pt : 500/503\n",
    "# bertWWM_09_01_54.pt : 501/503"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "cross = nn.BCELoss()\n",
    "x = torch.Tensor([0.3,0.2,0.1])\n",
    "y = torch.Tensor([1,0,0])\n",
    "print(x.shape,y.shape)\n",
    "z = cross(x,y)\n",
    "z\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
