{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_BIN",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eric88525/Tbrain_NLP_game/blob/master/train_BIN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnhG9bWCmSAq",
        "colab_type": "text"
      },
      "source": [
        "# 洗錢新聞分類器"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37bMWL6D8zno",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "46762f50-aa6a-4fff-ea6c-7731bc782dd6"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Jul 31 18:30:03 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.57       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   71C    P8    11W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIjhvFyLXU9n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "32aba109-26cf-4d97-989c-88b01505a946"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/bin/bash: google-drive-ocamlfuse: command not found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5cWZWqHXVLc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "fdf120f8-4b2c-4171-c542-107a886b54e5"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install opencc\n",
        "!pip install pyprind\n",
        "!pip install zhon\n",
        "#!pip install -U ckiptagger[tfgpu,gdown]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: opencc in /usr/local/lib/python3.6/dist-packages (1.1.1.post1)\n",
            "Requirement already satisfied: pyprind in /usr/local/lib/python3.6/dist-packages (2.11.2)\n",
            "Requirement already satisfied: zhon in /usr/local/lib/python3.6/dist-packages (1.1.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02nMffqpeCRk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "outputId": "91e5b25e-7ead-44ff-d964-9ce5fb800478"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from transformers import *\n",
        "import pandas as pd\n",
        "import ast\n",
        "import copy\n",
        "import os\n",
        "from time import strftime,gmtime\n",
        "from opencc import OpenCC\n",
        "import pyprind\n",
        "#from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "import re\n",
        "from zhon.hanzi import non_stops,stops\n",
        "import numpy as np\n",
        "import random\n",
        "print(torch.cuda.is_available())\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "Fri Jul 31 18:30:26 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.57       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   66C    P8    11W /  70W |     10MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imxxfLbnrR5d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "5ad2fc4b-9425-41e4-81ba-5993659b5f5b"
      },
      "source": [
        "cd ./drive/My Drive/Colab Notebooks/TBrain/nosep"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/TBrain/nosep\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deUCQ8WkzX81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval(pred, ans):\n",
        "    if bool(pred) is not bool(ans):\n",
        "        return 0\n",
        "    elif not pred and not ans:\n",
        "        return 1\n",
        "    else:\n",
        "        pred = set(pred)\n",
        "        ans = set(ans)\n",
        "        interaction_len = len(pred & ans)\n",
        "        if interaction_len == 0:\n",
        "            return 0\n",
        "\n",
        "        pred_len = len(pred)\n",
        "        ans_len = len(ans)\n",
        "        return 2 / (pred_len / interaction_len + ans_len / interaction_len)\n",
        "\n",
        "\n",
        "def eval_all(pred_list, ans_list):\n",
        "    assert len(pred_list) == len(ans_list)\n",
        "    return sum(eval(p, a) for p, a in zip(pred_list, ans_list)) / len(pred_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWpwu856XTO9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class mydata_nosep():\n",
        "    def __init__(self,train_path,test_path):\n",
        "\n",
        "        self.cc = OpenCC('t2s')\n",
        "\n",
        "        bin = lambda p: 1 if len(p) !=0 else 0\n",
        "\n",
        "        train_data = pd.read_csv(train_path)\n",
        "        test_data = pd.read_csv(test_path)\n",
        "\n",
        "        # CONTENT / bin\n",
        "        train_x = [ self.clean_string(i) for i in train_data['full_content'].values.tolist()]\n",
        "        train_y = [ bin(self.name_list(i)) for i in train_data['name'].values.tolist()]\n",
        "\n",
        "        # CONTENT / bin\n",
        "        test_x = [self.clean_string(i) for i in test_data['full_content'].values.tolist()]\n",
        "        test_y = [ bin(self.name_list(i)) for i in test_data['name'].values.tolist() ]\n",
        "\n",
        "        self.train = [] # CONTENT / NAME / (CKIP-NAME)\n",
        "        self.test = []  # CONTENT / NAME\n",
        "\n",
        "        qs = 0 \n",
        "        for i in range(len(train_x)): \n",
        "          self.train.append([train_x[i],train_y[i]])\n",
        "        \n",
        "        for i in range(len(test_x)):\n",
        "          self.test.append([test_x[i],test_y[i]])\n",
        "\n",
        "\n",
        "        print(f'Train: {len(self.train)} Test: {len(self.test)}')\n",
        "\n",
        "    def name_list(self,name_list_str):\n",
        "        ls = ast.literal_eval(name_list_str)\n",
        "        res = []\n",
        "        cc = OpenCC('t2s')\n",
        "        for n in ls:\n",
        "          res.append(cc.convert(n))\n",
        "        return set(res)        \n",
        "    def clean_string(self,content):\n",
        "      content = content.replace('\\n','。').replace('\\t','，').replace('!', '！').replace('?', '？')# erease white space cause English name error\n",
        "      content = re.sub(\"[+\\.\\/_,$%●▼►^*(+\\\"\\']+|[+——~@#￥%……&*（）★]\", \"\",content)\n",
        "      content = re.sub(r\"[%s]+\" %stops, \"。\",content)\n",
        "      content = self.cc.convert(content)\n",
        "      return content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OGS78UM3NW9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class TrainDataset(Dataset):\n",
        "    def __init__(self,data,model_type):\n",
        "      self.data = data\n",
        "      self.tokenizer = BertTokenizer.from_pretrained(model_type)\n",
        "        \n",
        "    def __getitem__(self,idx):\n",
        "      paragraph ,label = self.data[idx][0],self.data[idx][1]\n",
        "      token_tensor = self.tokenizer.encode_plus(paragraph,paragraph,max_length=512,truncation=True,pad_to_max_length=True)\n",
        "      label_tensor = torch.Tensor([label])\n",
        "      # token / segment / mask / label\n",
        "      return  torch.tensor(token_tensor['input_ids']), torch.tensor( token_tensor['token_type_ids']) , torch.tensor( token_tensor['attention_mask'] ),label_tensor\n",
        "    def __len__(self):\n",
        "      return len(self.data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Egtz9KVfigNH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "c93e337c-86e7-4c5f-eb9a-7ef3c86fd8ae"
      },
      "source": [
        "\"\"\"\n",
        "class bertwwmBIN(nn.Module):\n",
        "    def __init__(self,model_name,config):\n",
        "        super(bertwwmBIN,self).__init__()\n",
        "        self.bert_model = BertModel.from_pretrained(model_name)\n",
        "        self.bi_decoder = nn.Sequential(\n",
        "            nn.Linear(config.hidden_size,config.hidden_size)\n",
        "            ,nn.Dropout(0.1)\n",
        "            ,nn.ReLU()\n",
        "            ,nn.Linear(config.hidden_size,2)\n",
        "        )\n",
        "    def start(self):\n",
        "      nn.init.xavier_uniform_(self.bi_decoder[0].weight)\n",
        "      nn.init.constant_(self.bi_decoder[0].bias, 0)\n",
        "      nn.init.xavier_uniform_(self.bi_decoder[3].weight)\n",
        "      nn.init.constant_(self.bi_decoder[3].bias, 0)\n",
        "\n",
        "    def forward(self,input_ids=None,attention_mask=None,token_type_ids=None):\n",
        "        cls = self.bert_model(input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids)[1]\n",
        "        binary = self.bi_decoder(cls)\n",
        "        return binary\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nclass bertwwmBIN(nn.Module):\\n    def __init__(self,model_name,config):\\n        super(bertwwmBIN,self).__init__()\\n        self.bert_model = BertModel.from_pretrained(model_name)\\n        self.bi_decoder = nn.Sequential(\\n            nn.Linear(config.hidden_size,config.hidden_size)\\n            ,nn.Dropout(0.1)\\n            ,nn.ReLU()\\n            ,nn.Linear(config.hidden_size,2)\\n        )\\n    def start(self):\\n      nn.init.xavier_uniform_(self.bi_decoder[0].weight)\\n      nn.init.constant_(self.bi_decoder[0].bias, 0)\\n      nn.init.xavier_uniform_(self.bi_decoder[3].weight)\\n      nn.init.constant_(self.bi_decoder[3].bias, 0)\\n\\n    def forward(self,input_ids=None,attention_mask=None,token_type_ids=None):\\n        cls = self.bert_model(input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids)[1]\\n        binary = self.bi_decoder(cls)\\n        return binary\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54Lvy0qUjJHE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class bertwwmBIN(nn.Module):\n",
        "    def __init__(self,model_name,config):\n",
        "        super(bertwwmBIN,self).__init__()\n",
        "        self.bert_model = BertModel.from_pretrained(model_name)\n",
        "        self.bi_decoder = nn.Sequential(\n",
        "            nn.Linear(config.hidden_size,1)\n",
        "            ,nn.Sigmoid()\n",
        "        )\n",
        "    def start(self):\n",
        "      nn.init.xavier_uniform_(self.bi_decoder[0].weight)\n",
        "      nn.init.constant_(self.bi_decoder[0].bias, 0)\n",
        "      nn.init.xavier_uniform_(self.bi_decoder[3].weight)\n",
        "      nn.init.constant_(self.bi_decoder[3].bias, 0)\n",
        "\n",
        "    def forward(self,input_ids=None,attention_mask=None,token_type_ids=None):\n",
        "        cls = self.bert_model(input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids)[1]\n",
        "        binary = self.bi_decoder(cls)\n",
        "        return binary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrCj11hNlBXC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class bertwwmQA(nn.Module):\n",
        "    def __init__(self,model_name,config):\n",
        "        super(bertwwmQA,self).__init__()\n",
        "        #self.bert_model = BertModel.from_pretrained(\"hfl/chinese-roberta-wwm-ext-large\")\n",
        "        \n",
        "        self.bert_model = BertModel.from_pretrained(model_name,config = config)\n",
        "        self.bi_decoder = nn.Sequential(\n",
        "            nn.Linear(config.hidden_size,config.hidden_size)\n",
        "            ,nn.Dropout(0.1)\n",
        "            ,nn.ReLU()\n",
        "            ,nn.Linear(config.hidden_size,2)\n",
        "        )\n",
        "        #self.start()\n",
        "\n",
        "    def start(self):\n",
        "      nn.init.xavier_uniform_(self.bi_decoder[0].weight)\n",
        "      nn.init.constant_(self.bi_decoder[0].bias, 0)\n",
        "      nn.init.xavier_uniform_(self.bi_decoder[3].weight)\n",
        "      nn.init.constant_(self.bi_decoder[3].bias, 0)\n",
        "\n",
        "    def forward(self,input_ids=None,attention_mask=None,token_type_ids=None):\n",
        "       # print(f'receive input_ids {input_ids}')\n",
        "\n",
        "        cls = self.bert_model(input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids)[1]\n",
        "        binary = self.bi_decoder(cls)\n",
        "        return binary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeU_ooqwpN8T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        },
        "outputId": "7d736716-e3a0-4034-f412-6ddcec595f97"
      },
      "source": [
        "\"\"\"\n",
        "def test(model,data,device,model_name):\n",
        "    print('Testing...')\n",
        "    test_num = len(data.test)\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "    model.eval()\n",
        "    print(f'Testing data: {test_num}')\n",
        "    all,acc = 0,0\n",
        "    with torch.no_grad():\n",
        "      for content,label in data.test:\n",
        "        all += 1\n",
        "        if len(content)>512:\n",
        "          content = content[:512] \n",
        "        token_tensor = tokenizer.encode_plus(str(content),max_length=512,truncation=True,pad_to_max_length=True)\n",
        "        token = torch.tensor(token_tensor['input_ids']).unsqueeze(0).to(device)\n",
        "        segment = torch.tensor( token_tensor['token_type_ids']).unsqueeze(0).to(device)\n",
        "        mask = torch.tensor( token_tensor['attention_mask'] ).unsqueeze(0).to(device)\n",
        "        pred = model(input_ids=token,attention_mask=mask,token_type_ids=segment).argmax(dim=1)\n",
        "        print(pred)\n",
        "        if pred.item() == 1 and label == 1:\n",
        "          acc = acc +1\n",
        "        elif pred.item() == 0 and label == 0:\n",
        "          acc = acc +1\n",
        "    print(f'ALL: {all} ACC: {acc}')       \n",
        "    return all,acc\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\ndef test(model,data,device,model_name):\\n    print('Testing...')\\n    test_num = len(data.test)\\n    tokenizer = BertTokenizer.from_pretrained(model_name)\\n    model.eval()\\n    print(f'Testing data: {test_num}')\\n    all,acc = 0,0\\n    with torch.no_grad():\\n      for content,label in data.test:\\n        all += 1\\n        if len(content)>512:\\n          content = content[:512] \\n        token_tensor = tokenizer.encode_plus(str(content),max_length=512,truncation=True,pad_to_max_length=True)\\n        token = torch.tensor(token_tensor['input_ids']).unsqueeze(0).to(device)\\n        segment = torch.tensor( token_tensor['token_type_ids']).unsqueeze(0).to(device)\\n        mask = torch.tensor( token_tensor['attention_mask'] ).unsqueeze(0).to(device)\\n        pred = model(input_ids=token,attention_mask=mask,token_type_ids=segment).argmax(dim=1)\\n        print(pred)\\n        if pred.item() == 1 and label == 1:\\n          acc = acc +1\\n        elif pred.item() == 0 and label == 0:\\n          acc = acc +1\\n    print(f'ALL: {all} ACC: {acc}')       \\n    return all,acc\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9-BUq35ET6Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model,data,device,model_name):\n",
        "    print('Testing...')\n",
        "    test_num = len(data.test)\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "    model.eval()\n",
        "    print(f'Testing data: {test_num}')\n",
        "    all,acc = 0,0\n",
        "    with torch.no_grad():\n",
        "      for content,label in data.test:\n",
        "        all += 1\n",
        "        if len(content)>512:\n",
        "          content = content[:512] \n",
        "        token_tensor = tokenizer.encode_plus(str(content),max_length=512,truncation=True,pad_to_max_length=True)\n",
        "        token = torch.tensor(token_tensor['input_ids']).unsqueeze(0).to(device)\n",
        "        segment = torch.tensor( token_tensor['token_type_ids']).unsqueeze(0).to(device)\n",
        "        mask = torch.tensor( token_tensor['attention_mask'] ).unsqueeze(0).to(device)\n",
        "        pred = model(input_ids=token,attention_mask=mask,token_type_ids=segment)\n",
        "        print(f'P {pred.item():.2f} L {label}')\n",
        "        if pred.item() >0.4 and label == 1:\n",
        "          acc = acc +1\n",
        "        elif pred.item() < 0.4 and label == 0:\n",
        "          acc = acc +1\n",
        "    print(f'ALL: {all} ACC: {acc}')       \n",
        "    return all,acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrFxvpVYYQe2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        },
        "outputId": "b892bf55-65f5-48ee-f167-ee7fcab87d10"
      },
      "source": [
        "\"\"\"\n",
        "def train(trainLoader,w_d,lr_rate,device,model_type,epoches):\n",
        "  config = BertConfig.from_pretrained(model_type, output_hidden_states=True)\n",
        "  model = bertwwmBIN(model_type,config).to(device)\n",
        "  parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "  optimizer = AdamW(parameters, lr=lr_rate, weight_decay=w_d)\n",
        "  #criterion = nn.BCELoss()\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  model.train()\n",
        "  for e in range(epoches):\n",
        "    loss,i = 0,0\n",
        "    for data in trainLoader:\n",
        "      i+=1\n",
        "      token , segment , mask , label = [ item.to(device) for item in data]\n",
        "      pred = torch.softmax(model(input_ids=token,attention_mask=mask,token_type_ids=segment),dim=-1)\n",
        "      \n",
        "      label = label.reshape(label.shape[0])\n",
        "     # print(pred)\n",
        "     # print(label)\n",
        "\n",
        "     # print(pred.shape,label.shape)\n",
        "      batch_loss = criterion(pred,label.long())\n",
        "      loss += batch_loss\n",
        "     # print(batch_loss)\n",
        "      batch_loss.backward()\n",
        "     # print(f'Epoch {e} Batch {i} Loss is {batch_loss}')\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "     # print(f'Loss {batch_loss}')\n",
        "    print(f'Epoches: {e} Loss {loss} AVG: {loss/i}')\n",
        "  best_model = copy.deepcopy(model.state_dict())\n",
        "  return best_model\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\ndef train(trainLoader,w_d,lr_rate,device,model_type,epoches):\\n  config = BertConfig.from_pretrained(model_type, output_hidden_states=True)\\n  model = bertwwmBIN(model_type,config).to(device)\\n  parameters = filter(lambda p: p.requires_grad, model.parameters())\\n  optimizer = AdamW(parameters, lr=lr_rate, weight_decay=w_d)\\n  #criterion = nn.BCELoss()\\n  criterion = nn.CrossEntropyLoss()\\n  model.train()\\n  for e in range(epoches):\\n    loss,i = 0,0\\n    for data in trainLoader:\\n      i+=1\\n      token , segment , mask , label = [ item.to(device) for item in data]\\n      pred = torch.softmax(model(input_ids=token,attention_mask=mask,token_type_ids=segment),dim=-1)\\n      \\n      label = label.reshape(label.shape[0])\\n     # print(pred)\\n     # print(label)\\n\\n     # print(pred.shape,label.shape)\\n      batch_loss = criterion(pred,label.long())\\n      loss += batch_loss\\n     # print(batch_loss)\\n      batch_loss.backward()\\n     # print(f'Epoch {e} Batch {i} Loss is {batch_loss}')\\n      optimizer.step()\\n      optimizer.zero_grad()\\n     # print(f'Loss {batch_loss}')\\n    print(f'Epoches: {e} Loss {loss} AVG: {loss/i}')\\n  best_model = copy.deepcopy(model.state_dict())\\n  return best_model\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9A0oKVxfE-pe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(trainLoader,w_d,lr_rate,device,model_type,epoches):\n",
        "  config = BertConfig.from_pretrained(model_type, output_hidden_states=True)\n",
        "  model = bertwwmBIN(model_type,config).to(device)\n",
        "  parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "  optimizer = AdamW(parameters, lr=lr_rate, weight_decay=w_d)\n",
        "  criterion = nn.BCELoss()\n",
        "  #criterion = nn.CrossEntropyLoss()\n",
        "  model.train()\n",
        "  for e in range(epoches):\n",
        "    loss,i = 0,0\n",
        "    for data in trainLoader:\n",
        "      i+=1\n",
        "      token , segment , mask , label = [ item.to(device) for item in data]\n",
        "      pred = model(input_ids=token,attention_mask=mask,token_type_ids=segment)\n",
        "      #print(f'Pred {pred} {pred.shape}')\n",
        "     # print(f'Label {label} {label.shape}')\n",
        "      batch_loss = criterion(pred,label)\n",
        "      loss += batch_loss\n",
        "     # print(batch_loss)\n",
        "      batch_loss.backward()\n",
        "     # print(f'Epoch {e} Batch {i} Loss is {batch_loss}')\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "     # print(f'Loss {batch_loss}')\n",
        "    print(f'Epoches: {e} Loss {loss} AVG: {loss/i}')\n",
        "  best_model = copy.deepcopy(model.state_dict())\n",
        "  return best_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5HADkK4DefY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "9e463fee-14f2-46b5-e1e4-6dd1ba334594"
      },
      "source": [
        "mydata = mydata_nosep('./data0/tbrain_train.csv','./data0/tbrain_test.csv') # 491 / 487\n",
        "#mydata = mydata_nosep('./data1/tbrain_train.csv','./data1/tbrain_test.csv') # 491 / 491\n",
        "#mydata = mydata_nosep('./data2/tbrain_train.csv','./data2/tbrain_test.csv') # 491 / 491\n",
        "#mydata = mydata_nosep('./data3/tbrain_train.csv','./data3/tbrain_test.csv') # 491 / 491"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train: 4426 Test: 491\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xfxWQASyQFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
        "mode = 'testBIN'\n",
        "model_type = 'hfl/chinese-roberta-wwm-ext'\n",
        "#model_type = \"hfl/chinese-bert-wwm\"\n",
        "torch.cuda.empty_cache()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1F3jttAwYQom",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "da623f31-9724-40bb-e4bf-ed274f912640"
      },
      "source": [
        "if mode == 'trainBIN':\n",
        "  train_ds = TrainDataset(mydata.train,model_type)\n",
        "  trainLoader = DataLoader(train_ds, batch_size=8)\n",
        "  config = BertConfig.from_pretrained(model_type,output_hidden_states=True)\n",
        "  print('Train BIN')\n",
        "  best_model = train(trainLoader,0.001,1e-5,device,model_type,5)\n",
        "  if not os.path.exists('saved_models'):\n",
        "    os.makedirs('saved_models')    \n",
        "  modeltime = strftime('%H_%M_%S', gmtime()) \n",
        "  modelname = 'bertWWM_BIN_'+ modeltime\n",
        "  torch.save(best_model, f'saved_models/{modelname}.pt')\n",
        "  print(f'Train end, model name is {modelname}.pt')\n",
        "\n",
        "elif mode == 'testBIN':\n",
        "  print('testbin')\n",
        "  #model_type = 'hfl/chinese-roberta-wwm-ext'\n",
        "  #modelname = 'bertWWM_BIN_09_05_44.pt'\n",
        "  modelname = 'bertWWM_BIN_18_21_15.pt'\n",
        "  config = BertConfig.from_pretrained(model_type,output_hidden_states=True)\n",
        "  test_model = bertwwmBIN(model_type,config).to(device)\n",
        "  test_model.load_state_dict(torch.load(f'saved_models/{modelname}'))\n",
        "  test(test_model,mydata,device,model_type)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "testbin\n",
            "Testing...\n",
            "Testing data: 491\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 1.00 L 1\n",
            "P 0.00 L 0\n",
            "P 1.00 L 1\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.95 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 1.00 L 1\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.34 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 1.00 L 1\n",
            "P 1.00 L 1\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 1.00 L 1\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 1.00 L 1\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 1.00 L 1\n",
            "P 1.00 L 1\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 1.00 L 1\n",
            "P 0.00 L 0\n",
            "P 1.00 L 1\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 1.00 L 1\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 1.00 L 1\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 1.00 L 1\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 1.00 L 1\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.02 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 1.00 L 1\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 1.00 L 1\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 1.00 L 1\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 1.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 1.00 L 1\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.03 L 1\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.01 L 0\n",
            "P 0.00 L 0\n",
            "P 1.00 L 1\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 1.00 L 1\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 1.00 L 1\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 1.00 L 1\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 1.00 L 1\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 1.00 L 1\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 1.00 L 1\n",
            "P 0.07 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 1.00 L 1\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 1.00 L 1\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 1.00 L 1\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 1.00 L 1\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 1.00 L 1\n",
            "P 1.00 L 1\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 1.00 L 1\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.72 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 1.00 L 1\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 1.00 L 1\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 1.00 L 1\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "P 0.00 L 0\n",
            "ALL: 491 ACC: 487\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O25agBgLg62Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#RoBERTa-wwm-ext-large\thfl/chinese-roberta-wwm-ext-large\n",
        "#RoBERTa-wwm-ext\thfl/chinese-roberta-wwm-ext\n",
        "#BERT-wwm-ext\thfl/chinese-bert-wwm-ext\n",
        "#BERT-wwm\thfl/chinese-bert-wwm\n",
        "#RBT3\thfl/rbt3\n",
        "#RBTL3\thfl/rbtl3"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}