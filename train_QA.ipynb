{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_QA",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eric88525/Tbrain_NLP_game/blob/master/train_QA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIjhvFyLXU9n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "65e53e78-c567-4e23-e881-a0684c3d2218"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/bin/bash: google-drive-ocamlfuse: command not found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5cWZWqHXVLc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 965
        },
        "outputId": "454b32e5-6133-4ca5-e448-0e8a32ddd22c"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install opencc\n",
        "!pip install pyprind\n",
        "!pip install zhon\n",
        "!pip install -U ckiptagger[tfgpu,gdown]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: opencc in /usr/local/lib/python3.6/dist-packages (1.1.1.post1)\n",
            "Requirement already satisfied: pyprind in /usr/local/lib/python3.6/dist-packages (2.11.2)\n",
            "Requirement already satisfied: zhon in /usr/local/lib/python3.6/dist-packages (1.1.5)\n",
            "Requirement already up-to-date: ckiptagger[gdown,tfgpu] in /usr/local/lib/python3.6/dist-packages (0.1.1)\n",
            "Requirement already satisfied, skipping upgrade: gdown; extra == \"gdown\" in /usr/local/lib/python3.6/dist-packages (from ckiptagger[gdown,tfgpu]) (3.6.4)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-gpu<2,>=1.13.1; extra == \"tfgpu\" in /usr/local/lib/python3.6/dist-packages (from ckiptagger[gdown,tfgpu]) (1.15.3)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from gdown; extra == \"gdown\"->ckiptagger[gdown,tfgpu]) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from gdown; extra == \"gdown\"->ckiptagger[gdown,tfgpu]) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from gdown; extra == \"gdown\"->ckiptagger[gdown,tfgpu]) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2,>=1.13.1; extra == \"tfgpu\"->ckiptagger[gdown,tfgpu]) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2,>=1.13.1; extra == \"tfgpu\"->ckiptagger[gdown,tfgpu]) (3.12.2)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2,>=1.13.1; extra == \"tfgpu\"->ckiptagger[gdown,tfgpu]) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2,>=1.13.1; extra == \"tfgpu\"->ckiptagger[gdown,tfgpu]) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2,>=1.13.1; extra == \"tfgpu\"->ckiptagger[gdown,tfgpu]) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2,>=1.13.1; extra == \"tfgpu\"->ckiptagger[gdown,tfgpu]) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2,>=1.13.1; extra == \"tfgpu\"->ckiptagger[gdown,tfgpu]) (1.15.1)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2,>=1.13.1; extra == \"tfgpu\"->ckiptagger[gdown,tfgpu]) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2,>=1.13.1; extra == \"tfgpu\"->ckiptagger[gdown,tfgpu]) (0.34.2)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2,>=1.13.1; extra == \"tfgpu\"->ckiptagger[gdown,tfgpu]) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2,>=1.13.1; extra == \"tfgpu\"->ckiptagger[gdown,tfgpu]) (0.2.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2,>=1.13.1; extra == \"tfgpu\"->ckiptagger[gdown,tfgpu]) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2,>=1.13.1; extra == \"tfgpu\"->ckiptagger[gdown,tfgpu]) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2,>=1.13.1; extra == \"tfgpu\"->ckiptagger[gdown,tfgpu]) (1.30.0)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2,>=1.13.1; extra == \"tfgpu\"->ckiptagger[gdown,tfgpu]) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gdown; extra == \"gdown\"->ckiptagger[gdown,tfgpu]) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gdown; extra == \"gdown\"->ckiptagger[gdown,tfgpu]) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gdown; extra == \"gdown\"->ckiptagger[gdown,tfgpu]) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gdown; extra == \"gdown\"->ckiptagger[gdown,tfgpu]) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu<2,>=1.13.1; extra == \"tfgpu\"->ckiptagger[gdown,tfgpu]) (49.1.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu<2,>=1.13.1; extra == \"tfgpu\"->ckiptagger[gdown,tfgpu]) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu<2,>=1.13.1; extra == \"tfgpu\"->ckiptagger[gdown,tfgpu]) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu<2,>=1.13.1; extra == \"tfgpu\"->ckiptagger[gdown,tfgpu]) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu<2,>=1.13.1; extra == \"tfgpu\"->ckiptagger[gdown,tfgpu]) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu<2,>=1.13.1; extra == \"tfgpu\"->ckiptagger[gdown,tfgpu]) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02nMffqpeCRk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "41468873-4440-4ce7-fbce-8990e81ef505"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from transformers import *\n",
        "import pandas as pd\n",
        "import ast\n",
        "import copy\n",
        "import os\n",
        "from time import strftime,gmtime\n",
        "from opencc import OpenCC\n",
        "import pyprind\n",
        "#from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "import re\n",
        "from zhon.hanzi import non_stops,stops\n",
        "import numpy as np\n",
        "import random\n",
        "print(torch.cuda.is_available())\n",
        "!nvidia-smi"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "Wed Jul 29 14:39:35 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.51.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8    10W /  70W |     10MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imxxfLbnrR5d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fd68fd46-f398-4e6d-d9b2-d6f54ec84f7a"
      },
      "source": [
        "cd ./drive/My Drive/Colab Notebooks/TBrain/nosep"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/TBrain/nosep\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deUCQ8WkzX81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval(pred, ans):\n",
        "    if bool(pred) is not bool(ans):\n",
        "        return 0\n",
        "    elif not pred and not ans:\n",
        "        return 1\n",
        "    else:\n",
        "        pred = set(pred)\n",
        "        ans = set(ans)\n",
        "        interaction_len = len(pred & ans)\n",
        "        if interaction_len == 0:\n",
        "            return 0\n",
        "\n",
        "        pred_len = len(pred)\n",
        "        ans_len = len(ans)\n",
        "        return 2 / (pred_len / interaction_len + ans_len / interaction_len)\n",
        "\n",
        "\n",
        "def eval_all(pred_list, ans_list):\n",
        "    assert len(pred_list) == len(ans_list)\n",
        "    return sum(eval(p, a) for p, a in zip(pred_list, ans_list)) / len(pred_list)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWpwu856XTO9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class mydata_nosep():\n",
        "    def __init__(self,train_path,test_path):\n",
        "\n",
        "        self.cc = OpenCC('t2s')\n",
        "        train_data = pd.read_csv(train_path)\n",
        "        test_data = pd.read_csv(test_path)\n",
        "        # CONTENT / NAME / CKIPNAME\n",
        "        train_x = [ self.clean_string(i) for i in train_data['full_content'].values.tolist()]\n",
        "        train_y = [ self.name_list(i) for i in train_data['name'].values.tolist()]\n",
        "        train_z = [ self.name_list(i) for i in train_data['ckip_names'].values.tolist() ]\n",
        "\n",
        "        # CONTENT / NAME\n",
        "        test_x = [self.clean_string(i) for i in test_data['full_content'].values.tolist()]\n",
        "        self.test_y = [self.name_list(i) for i in test_data['name'].values.tolist()]\n",
        "        #test_z = [self.name_list(i) for i in test_data['ckip_names'].values.tolist()]\n",
        "\n",
        "        self.train = [] # CONTENT / NAME / (CKIP-NAME)\n",
        "        self.test = []  # CONTENT / NAME\n",
        "\n",
        "        \n",
        "        qs = 0 \n",
        "        for i in range(len(train_x)):\n",
        "          if len(train_y[i]) == 0:\n",
        "            #self.train.append([train_x[i],[],[]])\n",
        "            continue\n",
        "          q = []\n",
        "          a = []\n",
        "          for n in train_y[i]:\n",
        "            qs += 1\n",
        "            self.train.append([train_x[i],n,1])\n",
        "          for n in train_z[i]-train_y[i]:\n",
        "            if len(n)==1: # remove the ckip name len is 1\n",
        "              continue\n",
        "            qs += 1\n",
        "            self.train.append([train_x[i],n,0])              \n",
        "\n",
        "        for i in range(len(test_x)):\n",
        "          self.test.append([test_x[i],self.test_y[i],])\n",
        "\n",
        "\n",
        "        print(f'Train: {len(self.train)} Test: {len(self.test)} Questions: {qs}')\n",
        "\n",
        "    def name_list(self,name_list_str):\n",
        "        ls = ast.literal_eval(name_list_str)\n",
        "        res = []\n",
        "        cc = OpenCC('t2s')\n",
        "        for n in ls:\n",
        "          res.append(cc.convert(n))\n",
        "        return set(res)        \n",
        "    def clean_string(self,content):\n",
        "      content = content.replace('\\n','。').replace('\\t','，').replace('!', '！').replace('?', '？')# erease white space cause English name error\n",
        "      content = re.sub(\"[+\\.\\/_,$%●▼►^*(+\\\"\\']+|[+——~@#￥%……&*（）★]\", \"\",content)\n",
        "      content = re.sub(r\"[%s]+\" %stops, \"。\",content)\n",
        "      content = self.cc.convert(content)\n",
        "      return content"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OGS78UM3NW9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class TrainDataset(Dataset):\n",
        "    def __init__(self,data,model_type):\n",
        "      self.data = data\n",
        "      self.tokenizer = BertTokenizer.from_pretrained(model_type)\n",
        "        \n",
        "    def __getitem__(self,idx):\n",
        "      question , paragraph ,label = self.data[idx][1] , self.data[idx][0],self.data[idx][2]\n",
        "      token_tensor = self.tokenizer.encode_plus(question,paragraph,max_length=512,truncation=True,pad_to_max_length=True)\n",
        "      label_tensor = torch.Tensor([label])\n",
        "      # token / segment / mask / label\n",
        "      return  torch.tensor(token_tensor['input_ids']), torch.tensor( token_tensor['token_type_ids']) , torch.tensor( token_tensor['attention_mask'] ),label_tensor\n",
        "    def __len__(self):\n",
        "      return len(self.data)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQbFhVJES5KB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "f2c3fc7b-3640-4164-94fc-28849f0ec308"
      },
      "source": [
        "\"\"\"\n",
        "class TrainDataset(Dataset):\n",
        "    def __init__(self,data,model_type):\n",
        "      self.data = data\n",
        "      self.tokenizer = BertTokenizer.from_pretrained(model_type)\n",
        "        \n",
        "    def __getitem__(self,idx):\n",
        "      question , paragraph ,label = self.data[idx][1] , self.data[idx][0],self.data[idx][2]\n",
        "      token_tensor = self.tokenizer.encode_plus(question ,paragraph,max_length=512,truncation=True,pad_to_max_length=True)\n",
        "      label_tensor = torch.Tensor([label])\n",
        "      # token / segment / mask / label\n",
        "      return  torch.tensor(token_tensor['input_ids']), torch.tensor( token_tensor['token_type_ids']) , torch.tensor( token_tensor['attention_mask'] ),label_tensor\n",
        "    def __len__(self):\n",
        "      return len(self.data)\n",
        "\"\"\""
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nclass TrainDataset(Dataset):\\n    def __init__(self,data,model_type):\\n      self.data = data\\n      self.tokenizer = BertTokenizer.from_pretrained(model_type)\\n        \\n    def __getitem__(self,idx):\\n      question , paragraph ,label = self.data[idx][1] , self.data[idx][0],self.data[idx][2]\\n      token_tensor = self.tokenizer.encode_plus(question ,paragraph,max_length=512,truncation=True,pad_to_max_length=True)\\n      label_tensor = torch.Tensor([label])\\n      # token / segment / mask / label\\n      return  torch.tensor(token_tensor['input_ids']), torch.tensor( token_tensor['token_type_ids']) , torch.tensor( token_tensor['attention_mask'] ),label_tensor\\n    def __len__(self):\\n      return len(self.data)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Egtz9KVfigNH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class bertwwm(nn.Module):\n",
        "    def __init__(self,model_name):\n",
        "        super(bertwwm,self).__init__()\n",
        "        self.bert_model = BertModel.from_pretrained(model_name)\n",
        "        self.bi_decoder = nn.Sequential(     \n",
        "            nn.Linear(768,1)\n",
        "            ,nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self,input_ids):\n",
        "        all_hidden_states, all_attentions = self.bert_model(input_ids)[-2:]\n",
        "        binary = self.bi_decoder(all_attentions).reshape(1)\n",
        "        return binary"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nX2TV4G7aMb0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class bertwwmQA(nn.Module):\n",
        "    def __init__(self,model_name,config):\n",
        "        super(bertwwmQA,self).__init__()\n",
        "        #self.bert_model = BertModel.from_pretrained(\"hfl/chinese-roberta-wwm-ext-large\")\n",
        "        \n",
        "        self.bert_model = BertModel.from_pretrained(model_name,config = config)\n",
        "        self.bi_decoder = nn.Sequential(\n",
        "            nn.Linear(config.hidden_size,config.hidden_size)\n",
        "            ,nn.Dropout(0.1)\n",
        "            ,nn.ReLU()\n",
        "            ,nn.Linear(config.hidden_size,2)\n",
        "        )\n",
        "        #self.start()\n",
        "\n",
        "    def start(self):\n",
        "      nn.init.xavier_uniform_(self.bi_decoder[0].weight)\n",
        "      nn.init.constant_(self.bi_decoder[0].bias, 0)\n",
        "      nn.init.xavier_uniform_(self.bi_decoder[3].weight)\n",
        "      nn.init.constant_(self.bi_decoder[3].bias, 0)\n",
        "\n",
        "    def forward(self,input_ids=None,attention_mask=None,token_type_ids=None):\n",
        "       # print(f'receive input_ids {input_ids}')\n",
        "\n",
        "        cls = self.bert_model(input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids)[1]\n",
        "        binary = self.bi_decoder(cls)\n",
        "        return binary"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IG5nVEQ9ArVy",
        "colab_type": "text"
      },
      "source": [
        "# 打包啦"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tKTLDDNySx-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class QAFilter():\n",
        "  def __init__(self,model_path): \n",
        "    \n",
        "    # device\n",
        "    self.device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    \n",
        "    # model & tokenizer\n",
        "    model_type = 'hfl/chinese-roberta-wwm-ext'\n",
        "    config = BertConfig.from_pretrained(model_type,output_hidden_states=True)\n",
        "    self.tokenizer = BertTokenizer.from_pretrained(model_type)\n",
        "    self.model = bertwwmQA(model_type,config).to(self.device)\n",
        "    self.model.load_state_dict(torch.load(model_path)) \n",
        "  \n",
        "    # 繁簡轉換\n",
        "    self.c2tw = OpenCC('s2t') # china to tw\n",
        "    self.tw2c = OpenCC('t2s') # tw to china\n",
        "\n",
        "  def filter(self,content,name_list):\n",
        "    # name_list : a list like: ['蔡英文','陳水扁']\n",
        "    # content  : a new \"繁體新聞\"\n",
        "    # 會回傳像是 [1,0,0,1] 代表第0和3的人名是對的\n",
        "    content = self.tw2c.convert(content)\n",
        "    pred_nlist = []\n",
        "    with torch.no_grad():\n",
        "      for n in name_list:\n",
        "        n = self.tw2c.convert(n)\n",
        "        token_tensor = self.tokenizer.encode_plus(str(n),str(content),max_length=512,truncation=True,pad_to_max_length=True)\n",
        "        token = torch.tensor(token_tensor['input_ids']).unsqueeze(0).to(self.device)\n",
        "        segment = torch.tensor( token_tensor['token_type_ids']).unsqueeze(0).to(self.device)\n",
        "        mask = torch.tensor( token_tensor['attention_mask'] ).unsqueeze(0).to(self.device)\n",
        "        pred = self.model(input_ids=token,attention_mask=mask,token_type_ids=segment).argmax(dim=1)\n",
        "        #print(n,pred.item())\n",
        "        pred_nlist.append(pred.item())\n",
        "    return pred_nlist"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbDQgNxV1nVf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4c594051-3833-4738-e19d-14a37dc9232b"
      },
      "source": [
        "mf = QAFilter('./saved_models/QA_Filter.pt')\n",
        "content = \"屏東縣前車城鄉長林俊豪於在2010年任職鄉長後利用招標機會收取不法回扣，經去年屏東地院重判有期徒刑17年，而在監委黃進郁、王品妮調查後，同樣認定林俊豪透過白手套收受廠商不法回扣、未依規定請假，並還包庇下屬違法兼職羊肉爐生意等，因此今公布通透過記名表決方式，以10票全數無異議彈劾，將送公務員懲戒委員會處理。   據彈劾文，林俊豪於擔任屏東縣車城鄉鄉長期間，在「社皆坑溪災害復建工程」標案，由吳宛財於2014年1月間向得標廠商收取工程款8％之回扣款計19萬元；在「保力村保力路巷道道路改善工程」標案，由吳宛財於2014年7月底指示得標廠商將工程款10％之回扣款24萬元交由蔡佳雅代為收受。據彈劾文，林俊豪所涉4件工程都是依公開之預算金額98折計算訂定工程底價，違反《政府採購法》規定。   此外，據彈劾文，林俊豪知道吳宛財與蔡佳雅共同經營之大山羊肉爐兼職並領取月薪4萬元卻未依法處置，違反《公務員服務法》規定。甚至林俊豪於鄉長任職期間，每日僅在辦公室1小時卻未依規定請假外出，違法授權吳宛財代蓋鄉長職章，未依法盡其綜理鄉政等職責。   黃進郁、王品妮調查後，於昨日提案至全院審查會，經監委李建冰、陳品亨、賴筱涵、蕭信宏、蕭宜珊、賴家賢、楊白萱、黃志茂、謝欣容、陳翠銘等10人投票，全數贊成彈劾。（張誠瑜／台北報導）\"\n",
        "pred = ['李建冰', '吳宛財', '黃進郁', '林俊豪', '蕭信宏', '蔡佳雅', '王品妮', '蕭宜珊']\n",
        "print(mf.filter(content,pred))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 1, 1, 1, 1, 0, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3yw4mNuDQ6f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0f31d0df-618a-41dc-8b0b-4230a3a07fee"
      },
      "source": [
        "content = \"【王惠文╱高雄報導】曾任「花媽」陳夢羽機要秘書的高市府前專委趙竹珍，因承辦世運行銷招標案收受廠商賄賂，4年前被判刑11年定讞，後因未到案執行遭通緝，但4年來他爽爽度日，不僅跑攤聚餐吃香喝辣，還高調將趴趴走的照片po臉書炫耀，直到《蘋果》昨接獲讀者爆料揭發此事，趙竹珍才火速關閉臉書。  據了解，趙竹珍一向被視為「花媽」陳夢羽愛將，陳夢羽任高雄市社會局長時，趙竹珍就擔任機要秘書；陳夢羽轉任勞委會主委時，趙則為勞保局總經理秘書；陳夢羽入主高市後，趙竹珍先是擔任市長室機要，後轉任新聞處專委。 趙竹珍被控於2009年間，擔任高市政府工務局專門委員，借調高雄市府新聞處擔任新聞處處長室機要秘書，他奉派支援世運基金會行銷公關部事務時，在世運基金會辦理「國內廣告案」招標期間，獲新聞處長指派兼任評審委員，卻以需廠商贊助活動費等藉口，勒索台灣電通黃姓承辦人交付賄款，並收受180萬元。 高雄地檢署依貪污等罪將趙起訴後，2011年5月一審重判他19年，趙上訴後，更一審改判他9年6月，直到2014年7月最高院駁回上訴定讞。由於趙竹珍另涉3案都判有罪，2014年8月，法院將其4罪合併定執行刑11年，但高雄地檢署通知趙竹珍到案執行，趙均不理會、遲未到案。  友留言：自由真好 離譜的是，趙竹珍被通緝後，不但四處趴趴走、還高調上餐館聚餐，讀者爆料指，上個月曾在高雄鹹水煙海鮮餐廳，看到趙竹珍和綠委助理爽聚餐，《蘋果》記者察看趙的臉書，果真發現他三不五時就上臉書，將趴趴走的照片po文、貼照片，知情好友看完後還羨慕地留言讚嘆：「自由真好」，似乎大家都知道趙竹珍在哪裡，只有檢警抓不到他。 爆料讀者質疑趙是因有政黨關係才免入獄，嘲諷趙「背後有山真好！」對此高雄地檢署指出，趙是在2014年8月遭判刑定讞，檢方收到確定判決後隨即發出執行通知，但趙男遲未到案，隔年6月已通緝，目前仍持續通緝中。   據彈劾文，林俊豪於擔任屏東縣車城鄉鄉長期間，在「社皆坑溪災害復建工程」標案，由吳宛財於2014年1月間向得標廠商收取工程款8％之回扣款計19萬元；在「保力村保力路巷道道路改善工程」標案，由吳宛財於2014年7月底指示得標廠商將工程款10％之回扣款24萬元交由蔡佳雅代為收受。據彈劾文，林俊豪所涉4件工程都是依公開之預算金額98折計算訂定工程底價，違反《政府採購法》規定。   此外，據彈劾文，林俊豪知道吳宛財與蔡佳雅共同經營之大山羊肉爐兼職並領取月薪4萬元卻未依法處置，違反《公務員服務法》規定。甚至林俊豪於鄉長任職期間，每日僅在辦公室1小時卻未依規定請假外出，違法授權吳宛財代蓋鄉長職章，未依法盡其綜理鄉政等職責。   黃進郁、王品妮調查後，於昨日提案至全院審查會，經監委李建冰、陳品亨、賴筱涵、蕭信宏、蕭宜珊、賴家賢、楊白萱、黃志茂、謝欣容、陳翠銘等10人投票，全數贊成彈劾。（張誠瑜／台北報導）\"\n",
        "pred = ['陳夢羽', '趙竹珍']\n",
        "print(mf.filter(content,pred))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAYchj8iEEcb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e3f694c8-00e2-4e89-99f7-6c005f9de801"
      },
      "source": [
        "content = \"〔記者張誠瑜／台中報導〕男子林燕樺、陳容佳於2016年加入詐欺集團當車手，提款27次共18萬餘元被法辦，2人於彰化地方法院審理時，都承認是詐欺車手，一審法官卻認為，檢警沒有找到中國的被害人，沒有證據可證明2人提領的是詐欺贓款，也有可能是不違法的一般提款，竟判決2人無罪，台中高分院二審時，2人改口說，領的是賭博贏得的錢，不是詐欺贓款，二審法官卻認為，如果是賭博彩金，何需聘僱車手領款？又何需用多個銀行帳戶迂迴提款？「一般事理常情」即可認定是詐欺贓款，依加重詐欺罪改判2人各1年3月徒刑。判決書指出，林、陳2人加入「小胖」詐欺集團，酬勞是領取贓款的1％，「小胖」於2016年11至12月，交給2人17張中國的銀聯卡與密碼，2人於12月12日，在土地銀行彰化分行的兩台提款機領款27次，共領走18萬8100元，於指定地點交給「小胖」，土銀彰化分行發現異常，通報彰化警分局查獲2人法辦。彰化地檢署偵查時，雖指示警方調查位於中國被害人資料，但因兩岸隔閡，且贓款被轉帳到多個人頭帳戶，警方查不到被害人身分，無從得知詐欺過程，也沒有抓到「小胖」，彰化地院審理時，2人承認是詐欺集團車手，陳男還供稱，他知道領的錢，是來自中國的詐欺所得。不料，一審卻認為，檢察官提出的證據，無法證明「小胖」集團從事詐欺，有可能是從事地下匯兌或洗錢等犯罪行為，也有可能只是單純提款，另外，被害人是誰？何時何地受騙？受騙金額多少？也都無從得知，2人雖自白是詐欺車手，卻沒有證據可證明，2人領的是詐欺贓款，據此判決2人無罪。彰化地檢署上訴二審，台中高分院審理時，2人都改口，林男宣稱，領的是「小胖」網路賭博贏得的錢，陳男則說，他不知錢的來源，二審法官認為，2人一天持17張銀聯卡提領27次，如果是「小胖」賭博輸贏所得，何必使用多個帳戶，還迂迴用聯銀卡分次小額提領？又何必花錢聘請2人當車手？依一般事理常情，這些錢應屬詐欺贓款，據此撤銷原審無罪判決，改判2人各1年3月徒刑，可再上訴。\"\n",
        "pred = ['林燕樺', '陳容佳']\n",
        "print(mf.filter(content,pred))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLM11AaYDSzs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeU_ooqwpN8T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model,data,device,ws,pos,ner,model_name):\n",
        "    #b_modelname = 'bertWWM_BI_16_27_53.pt'\n",
        "    #bin_model = bertwwm().to(device)\n",
        "    #bin_model.load_state_dict(torch.load(f'saved_models/{b_modelname}'))\n",
        "    print('Testing...')\n",
        "    test_num = len(data.test)\n",
        "    #criterion = nn.BCELoss()\n",
        "    #b_tokenizer = BertTokenizer.from_pretrained(\"hfl/chinese-bert-wwm\")\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "    model.eval()\n",
        "    print(f'Testing data: {test_num}')\n",
        "    c_to_tw = OpenCC('s2t') # chi to tw\n",
        "    tw_to_c = OpenCC('t2s')\n",
        "    label_count = 0\n",
        "    all_predlist = []\n",
        "    all_labellist = []\n",
        "   # all_labellist = data.test_y\n",
        "    with torch.no_grad():\n",
        "      for content,nlabel in data.test:\n",
        "        if len(nlabel) == 0:\n",
        "         # all_labellist.append([])\n",
        "          continue     \n",
        "       # print(f'test name:')\n",
        "        \n",
        "        if len(content)>512:\n",
        "          content = content[:512] \n",
        "        label_count += 1\n",
        "        ckip_nlist = []\n",
        "        pred_nlist = []\n",
        "        ws_results = ws([c_to_tw.convert(content)])\n",
        "        pos_results = pos(ws_results)\n",
        "        ner_results = ner(ws_results, pos_results)\n",
        "        for name in ner_results[0]:\n",
        "          if name[2] == 'PERSON':\n",
        "            ckip_nlist.append(tw_to_c.convert(name[3]))\n",
        "        ckip_nlist = set(ckip_nlist)\n",
        "        for ckip_name in ckip_nlist:\n",
        "          if len(ckip_name)<2:\n",
        "            continue\n",
        "          token_tensor = tokenizer.encode_plus(str(ckip_name),str(content),max_length=512,truncation=True,pad_to_max_length=True)\n",
        "          token = torch.tensor(token_tensor['input_ids']).unsqueeze(0).to(device)\n",
        "          segment = torch.tensor( token_tensor['token_type_ids']).unsqueeze(0).to(device)\n",
        "          mask = torch.tensor( token_tensor['attention_mask'] ).unsqueeze(0).to(device)\n",
        "          pred = model(input_ids=token,attention_mask=mask,token_type_ids=segment).argmax(dim=1)\n",
        "          if pred.item() == 1:\n",
        "            pred_nlist.append(ckip_name)\n",
        "       # pred_nlist = sorted(pred_nlist, key = lambda s: s[1])\n",
        "        all_labellist.append(nlabel)\n",
        "        all_predlist.append(pred_nlist)\n",
        "        print(f'CKIP:{ckip_nlist} PRED:{pred_nlist} LABEL:{nlabel}')\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "    return all_predlist,all_labellist"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtksFQ2ghOFB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def testBIN(model,data,device,ws,pos,ner,model_name):\n",
        "    print('Testing...')\n",
        "    acc = 0\n",
        "    total = len(data.test)\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "    with torch.no_grad():\n",
        "      for content,name in data.test:\n",
        "        input_ids = torch.tensor([tokenizer.encode(str(content),max_length=512,truncation=True,pad_to_max_length=True)]).to(device)   \n",
        "        b_pred = model(input_ids).to(device) \n",
        "        if b_pred.item() < 0.4 and len(name) == 0:\n",
        "          acc +=1\n",
        "        elif b_pred.item() > 0.4 and len(name):\n",
        "          acc +=1\n",
        "        print(f'C: {content[:20]} Name: {name} Pred: {b_pred.item()}')\n",
        "    print(f'ALL {total} ACC {acc}')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrFxvpVYYQe2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(trainLoader,w_d,lr_rate,device,model_type,epoches):\n",
        "  config = BertConfig.from_pretrained(model_type, output_hidden_states=True)\n",
        "  model = bertwwmQA(model_type,config).to(device)\n",
        "  parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "  optimizer = AdamW(parameters, lr=lr_rate, weight_decay=w_d)\n",
        "  #criterion = nn.BCELoss()\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  model.train()\n",
        "  for e in range(epoches):\n",
        "    loss,i = 0,0\n",
        "    for data in trainLoader:\n",
        "      i+=1\n",
        "      token , segment , mask , label = [ item.to(device) for item in data]\n",
        "      pred = torch.softmax(model(input_ids=token,attention_mask=mask,token_type_ids=segment),dim=-1)\n",
        "      \n",
        "      label = label.reshape(label.shape[0])\n",
        "     # print(pred)\n",
        "     # print(label)\n",
        "\n",
        "     # print(pred.shape,label.shape)\n",
        "      batch_loss = criterion(pred,label.long())\n",
        "      loss += batch_loss\n",
        "     # print(batch_loss)\n",
        "      batch_loss.backward()\n",
        "     # print(f'Epoch {e} Batch {i} Loss is {batch_loss}')\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "    print(f'Epoches: {e} Loss {loss} AVG: {loss/i}')\n",
        "  best_model = copy.deepcopy(model.state_dict())\n",
        "  return best_model"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GW7soANhhXJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UgMdGyyd0dx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "91ef8309-ccc5-4712-c7d1-eba71e7f0557"
      },
      "source": [
        "\"\"\"\n",
        "def trainBinary(data,w_d,lr_rate,device,model_type):\n",
        "  model = bertwwm(model_type).to(device)\n",
        "  parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "  optimizer = AdamW(parameters, lr=lr_rate, weight_decay=w_d)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  #criterion = nn.BCEWithLogitsLoss()\n",
        "  tokenizer = BertTokenizer.from_pretrained(model_type)\n",
        "  optimizer.zero_grad()\n",
        "  loss = 0\n",
        "  model.train()\n",
        "  for epoches in range(10):\n",
        "    print(f'Epoches: {epoches}')\n",
        "    i = 0\n",
        "    loss = 0\n",
        "    for content,question,ans in data.train: \n",
        "      i += 1\n",
        "      input_ids = torch.tensor([tokenizer.encode(str(content),max_length=512,truncation=True,pad_to_max_length=True)]).to(device)\n",
        "      pred = model(input_ids)\n",
        "      if len(question)!=0: \n",
        "        b_label = torch.Tensor([1]).to(device)\n",
        "      elif len(question) == 0:\n",
        "        b_label = torch.Tensor([0]).to(device)\n",
        "      a_loss = criterion(pred,b_label)\n",
        "      loss += a_loss\n",
        "      a_loss.backward()\n",
        "      del input_ids\n",
        "      del b_label\n",
        "      #print(f'i: {i} Content:{content[:10]} Q: {question} Label {b_label} Pred:{pred} Loss:{a_loss}')\n",
        "      if i % 4 == 0:\n",
        "        print(f'B LOSS is {loss/4}')\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        loss = 0\n",
        "  best_model = copy.deepcopy(model.state_dict())\n",
        "  return best_model\n",
        "\"\"\""
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\ndef trainBinary(data,w_d,lr_rate,device,model_type):\\n  model = bertwwm(model_type).to(device)\\n  parameters = filter(lambda p: p.requires_grad, model.parameters())\\n  optimizer = AdamW(parameters, lr=lr_rate, weight_decay=w_d)\\n  criterion = nn.CrossEntropyLoss()\\n  #criterion = nn.BCEWithLogitsLoss()\\n  tokenizer = BertTokenizer.from_pretrained(model_type)\\n  optimizer.zero_grad()\\n  loss = 0\\n  model.train()\\n  for epoches in range(10):\\n    print(f'Epoches: {epoches}')\\n    i = 0\\n    loss = 0\\n    for content,question,ans in data.train: \\n      i += 1\\n      input_ids = torch.tensor([tokenizer.encode(str(content),max_length=512,truncation=True,pad_to_max_length=True)]).to(device)\\n      pred = model(input_ids)\\n      if len(question)!=0: \\n        b_label = torch.Tensor([1]).to(device)\\n      elif len(question) == 0:\\n        b_label = torch.Tensor([0]).to(device)\\n      a_loss = criterion(pred,b_label)\\n      loss += a_loss\\n      a_loss.backward()\\n      del input_ids\\n      del b_label\\n      #print(f'i: {i} Content:{content[:10]} Q: {question} Label {b_label} Pred:{pred} Loss:{a_loss}')\\n      if i % 4 == 0:\\n        print(f'B LOSS is {loss/4}')\\n        optimizer.step()\\n        optimizer.zero_grad()\\n        loss = 0\\n  best_model = copy.deepcopy(model.state_dict())\\n  return best_model\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jpoz7eIbbRHE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYwVSH-TvsBu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model_type = 'hfl/chinese-bert-wwm'\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5HADkK4DefY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "outputId": "9f80f7c5-f02c-4c79-ad14-4c42cf920273"
      },
      "source": [
        "mydata = mydata_nosep('./data3/tbrain_train.csv','./data3/tbrain_test.csv')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.6/genericpath.py\u001b[0m in \u001b[0;36misfile\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 't2s.json'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-e40324ae2baf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmydata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmydata_nosep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data3/tbrain_train.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'./data3/tbrain_test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-4172b0decc71>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, train_path, test_path)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# CONTENT / NAME / CKIPNAME\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtrain_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full_content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mtrain_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ckip_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-4172b0decc71>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# CONTENT / NAME / CKIPNAME\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtrain_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full_content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mtrain_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ckip_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-4172b0decc71>\u001b[0m in \u001b[0;36mname_list\u001b[0;34m(self, name_list_str)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mliteral_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_list_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenCC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m't2s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m           \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/opencc/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opencc_share_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/genericpath.py\u001b[0m in \u001b[0;36misfile\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;34m\"\"\"Test whether a path is a regular file\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFRr64FirS8O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from ckiptagger import WS, POS, NER\n",
        "ws = WS(\"./data\")\n",
        "pos = POS(\"./data\")\n",
        "ner = NER(\"./data\")\n",
        "print('CKIP finish')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xfxWQASyQFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
        "mode = 'testqa'\n",
        "model_type = 'hfl/chinese-roberta-wwm-ext'\n",
        "torch.cuda.empty_cache()  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1F3jttAwYQom",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if mode == 'trainqa':\n",
        "  #model_type = 'hfl/chinese-bert-wwm'\n",
        "  train_ds = TrainDataset(mydata.train,model_type)\n",
        "  trainLoader = DataLoader(train_ds, batch_size=4)\n",
        "  config = BertConfig.from_pretrained(model_type,output_hidden_states=True)\n",
        "  print('Train QA')\n",
        "  best_model = train(trainLoader,0.001,1e-5,device,model_type,10)\n",
        "  if not os.path.exists('saved_models'):\n",
        "    os.makedirs('saved_models')    \n",
        "  modeltime = strftime('%H_%M_%S', gmtime()) \n",
        "  modelname = 'bertWWM_QA_'+ modeltime\n",
        "  torch.save(best_model, f'saved_models/{modelname}.pt')\n",
        "  print(f'Train end, model name is {modelname}.pt')\n",
        "\n",
        "elif mode == 'testqa':\n",
        "  print('Testqa')\n",
        "  model_type = 'hfl/chinese-roberta-wwm-ext'\n",
        "  modelname = 'bertWWM_QA_06_01_38.pt'\n",
        "  config = BertConfig.from_pretrained(model_type,output_hidden_states=True)\n",
        "\n",
        "  test_model = bertwwmQA(model_type,config).to(device)\n",
        "  test_model.load_state_dict(torch.load(f'saved_models/{modelname}'))\n",
        "  all_predlist,all_labellist = test(test_model,mydata,device,ws,pos,ner,model_type)\n",
        "  print(f'Testend point is {eval_all(all_predlist,all_labellist)}')\n",
        "# train binary classfier\n",
        "elif mode == 'trainbin':\n",
        "  model_type = 'hfl/chinese-bert-wwm'\n",
        "  best_model = trainBinary(data,0.005,5e-4,device,model_type)\n",
        "  if not os.path.exists('saved_models'):\n",
        "    os.makedirs('saved_models')    \n",
        "  modeltime = strftime('%H_%M_%S', gmtime()) \n",
        "  modelname = 'bertWWM_BIN_'+ modeltime\n",
        "  torch.save(best_model, f'saved_models/{modelname}.pt')\n",
        "  print(f'Train end, model name is {modelname}.pt')\n",
        "\n",
        "elif mode == 'testbin':\n",
        "  modelname = 'bertWWM_BIN_15_05_01.pt'\n",
        "  model_type = 'hfl/chinese-bert-wwm'\n",
        " # test_model = bertwwm(model_type).to(device)\n",
        "  test_model = bertwwm(model_type).to(device)\n",
        "  test_model.load_state_dict(torch.load(f'saved_models/{modelname}'))\n",
        "  \n",
        "  testBIN(test_model,data,device,ws,pos,ner,model_type)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O25agBgLg62Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#RoBERTa-wwm-ext-large\thfl/chinese-roberta-wwm-ext-large\n",
        "#RoBERTa-wwm-ext\thfl/chinese-roberta-wwm-ext\n",
        "#BERT-wwm-ext\thfl/chinese-bert-wwm-ext\n",
        "#BERT-wwm\thfl/chinese-bert-wwm\n",
        "#RBT3\thfl/rbt3\n",
        "#RBTL3\thfl/rbtl3"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}