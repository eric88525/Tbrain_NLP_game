{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_QA",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eric88525/Tbrain_NLP_game/blob/master/train_QA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIY7_QltfVUn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "102da79b-d272-4b0a-a9bf-1fa2ba031c91"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Aug  5 14:17:02 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.57       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIjhvFyLXU9n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "a07a459d-0445-423f-eea8-013d921a5a56"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "/bin/bash: google-drive-ocamlfuse: command not found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5cWZWqHXVLc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 945
        },
        "outputId": "fc372df2-7e72-4b41-f291-b38f84a019d5"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install opencc\n",
        "!pip install pyprind\n",
        "!pip install zhon\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 24.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 38.2MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 51.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=cf333b2804b5e4a3d3394fe60e86d42e4e1cef3c28a80ac31c303424e9bbd4e1\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n",
            "Collecting opencc\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/b4/24e677e135df130fc6989929dc3990a1ae19948daf28beb8f910b4f7b671/OpenCC-1.1.1.post1-py2.py3-none-manylinux1_x86_64.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 4.7MB/s \n",
            "\u001b[?25hInstalling collected packages: opencc\n",
            "Successfully installed opencc-1.1.1.post1\n",
            "Collecting pyprind\n",
            "  Downloading https://files.pythonhosted.org/packages/1e/30/e76fb0c45da8aef49ea8d2a90d4e7a6877b45894c25f12fb961f009a891e/PyPrind-2.11.2-py3-none-any.whl\n",
            "Installing collected packages: pyprind\n",
            "Successfully installed pyprind-2.11.2\n",
            "Collecting zhon\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/b0/c56c6079ad47c35a2341440818b6620de8c46a265ed690a51b1a4e5591bc/zhon-1.1.5.tar.gz (99kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 3.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: zhon\n",
            "  Building wheel for zhon (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for zhon: filename=zhon-1.1.5-cp36-none-any.whl size=84293 sha256=c68b8428e21f2aa483d65fbcec7d24486725d41c7d39e35775483a9a0826754c\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/93/5a/ad2f403c359ba996e33c21bf18611d921413df9740ede2fcf4\n",
            "Successfully built zhon\n",
            "Installing collected packages: zhon\n",
            "Successfully installed zhon-1.1.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02nMffqpeCRk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "9f7a2bf2-d823-45aa-f183-9f36f0200721"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from transformers import *\n",
        "import pandas as pd\n",
        "import ast\n",
        "import copy\n",
        "import os\n",
        "from time import strftime,gmtime\n",
        "from opencc import OpenCC\n",
        "import pyprind\n",
        "#from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "import re\n",
        "from zhon.hanzi import non_stops,stops\n",
        "import numpy as np\n",
        "import random\n",
        "print(torch.cuda.is_available())\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "Wed Aug  5 14:18:15 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.57       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P8     9W /  70W |     10MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imxxfLbnrR5d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b57ccf0a-7ab8-4edc-e75d-c511d1e83e7c"
      },
      "source": [
        "cd ./drive/My Drive/Colab Notebooks/TBrain/nosep/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/TBrain/nosep\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaZyVXJm5Coe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deUCQ8WkzX81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval(pred, ans):\n",
        "    if bool(pred) is not bool(ans):\n",
        "        return 0\n",
        "    elif not pred and not ans:\n",
        "        return 1\n",
        "    else:\n",
        "        pred = set(pred)\n",
        "        ans = set(ans)\n",
        "        interaction_len = len(pred & ans)\n",
        "        if interaction_len == 0:\n",
        "            return 0\n",
        "\n",
        "        pred_len = len(pred)\n",
        "        ans_len = len(ans)\n",
        "        return 2 / (pred_len / interaction_len + ans_len / interaction_len)\n",
        "\n",
        "\n",
        "def eval_all(pred_list, ans_list):\n",
        "    assert len(pred_list) == len(ans_list)\n",
        "    return sum(eval(p, a) for p, a in zip(pred_list, ans_list)) / len(pred_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWpwu856XTO9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "25a03112-aa7f-458f-80e9-5b9063a60721"
      },
      "source": [
        "\"\"\"\n",
        "class mydata_nosep():\n",
        "    def __init__(self,train_path,test_path):\n",
        "\n",
        "        self.cc = OpenCC('t2s')\n",
        "        train_data = pd.read_csv(train_path)\n",
        "        test_data = pd.read_csv(test_path)\n",
        "        # CONTENT / NAME / CKIPNAME\n",
        "        train_x = [ self.clean_string(i) for i in train_data['full_content'].values.tolist()]\n",
        "        train_y = [ self.name_list(i) for i in train_data['name'].values.tolist()]\n",
        "        train_z = [ self.name_list(i) for i in train_data['ckip_names'].values.tolist() ]\n",
        "\n",
        "        # CONTENT / NAME\n",
        "        test_x = [self.clean_string(i) for i in test_data['full_content'].values.tolist()]\n",
        "        self.test_y = [self.name_list(i) for i in test_data['name'].values.tolist()]\n",
        "        #test_z = [self.name_list(i) for i in test_data['ckip_names'].values.tolist()]\n",
        "\n",
        "        self.train = [] # CONTENT / NAME / (CKIP-NAME)\n",
        "        self.test = []  # CONTENT / NAME\n",
        "\n",
        "        \n",
        "        qs = 0 \n",
        "        for i in range(len(train_x)):\n",
        "          if len(train_y[i]) == 0:\n",
        "            #self.train.append([train_x[i],[],[]])\n",
        "            continue\n",
        "          q = []\n",
        "          a = []\n",
        "          for n in train_y[i]:\n",
        "            qs += 1\n",
        "            self.train.append([train_x[i],n,1])\n",
        "          for n in train_z[i]-train_y[i]:\n",
        "            if len(n)==1: # remove the ckip name len is 1\n",
        "              continue\n",
        "            qs += 1\n",
        "            self.train.append([train_x[i],n,0])              \n",
        "\n",
        "        for i in range(len(test_x)):\n",
        "          self.test.append([test_x[i],self.test_y[i],])\n",
        "\n",
        "\n",
        "        print(f'Train: {len(self.train)} Test: {len(self.test)} Questions: {qs}')\n",
        "\n",
        "    def name_list(self,name_list_str):\n",
        "        ls = ast.literal_eval(name_list_str)\n",
        "        res = []\n",
        "        cc = OpenCC('t2s')\n",
        "        for n in ls:\n",
        "          res.append(cc.convert(n))\n",
        "        return set(res)        \n",
        "    def clean_string(self,content):\n",
        "      content = content.replace('\\n','。').replace('\\t','，').replace('!', '！').replace('?', '？')# erease white space cause English name error\n",
        "      content = re.sub(\"[+\\.\\/_,$%●▼►^*(+\\\"\\']+|[+——~@#￥%……&*（）★]\", \"\",content)\n",
        "      content = re.sub(r\"[%s]+\" %stops, \"。\",content)\n",
        "      content = self.cc.convert(content)\n",
        "      return content\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nclass mydata_nosep():\\n    def __init__(self,train_path,test_path):\\n\\n        self.cc = OpenCC(\\'t2s\\')\\n        train_data = pd.read_csv(train_path)\\n        test_data = pd.read_csv(test_path)\\n        # CONTENT / NAME / CKIPNAME\\n        train_x = [ self.clean_string(i) for i in train_data[\\'full_content\\'].values.tolist()]\\n        train_y = [ self.name_list(i) for i in train_data[\\'name\\'].values.tolist()]\\n        train_z = [ self.name_list(i) for i in train_data[\\'ckip_names\\'].values.tolist() ]\\n\\n        # CONTENT / NAME\\n        test_x = [self.clean_string(i) for i in test_data[\\'full_content\\'].values.tolist()]\\n        self.test_y = [self.name_list(i) for i in test_data[\\'name\\'].values.tolist()]\\n        #test_z = [self.name_list(i) for i in test_data[\\'ckip_names\\'].values.tolist()]\\n\\n        self.train = [] # CONTENT / NAME / (CKIP-NAME)\\n        self.test = []  # CONTENT / NAME\\n\\n        \\n        qs = 0 \\n        for i in range(len(train_x)):\\n          if len(train_y[i]) == 0:\\n            #self.train.append([train_x[i],[],[]])\\n            continue\\n          q = []\\n          a = []\\n          for n in train_y[i]:\\n            qs += 1\\n            self.train.append([train_x[i],n,1])\\n          for n in train_z[i]-train_y[i]:\\n            if len(n)==1: # remove the ckip name len is 1\\n              continue\\n            qs += 1\\n            self.train.append([train_x[i],n,0])              \\n\\n        for i in range(len(test_x)):\\n          self.test.append([test_x[i],self.test_y[i],])\\n\\n\\n        print(f\\'Train: {len(self.train)} Test: {len(self.test)} Questions: {qs}\\')\\n\\n    def name_list(self,name_list_str):\\n        ls = ast.literal_eval(name_list_str)\\n        res = []\\n        cc = OpenCC(\\'t2s\\')\\n        for n in ls:\\n          res.append(cc.convert(n))\\n        return set(res)        \\n    def clean_string(self,content):\\n      content = content.replace(\\'\\n\\',\\'。\\').replace(\\'\\t\\',\\'，\\').replace(\\'!\\', \\'！\\').replace(\\'?\\', \\'？\\')# erease white space cause English name error\\n      content = re.sub(\"[+\\\\.\\\\/_,$%●▼►^*(+\"\\']+|[+——~@#￥%……&*（）★]\", \"\",content)\\n      content = re.sub(r\"[%s]+\" %stops, \"。\",content)\\n      content = self.cc.convert(content)\\n      return content\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMfC3wqN7CEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class mydata_nosep():\n",
        "    def __init__(self,train_path,test_path):\n",
        "\n",
        "        self.cc = OpenCC('t2s')\n",
        "        train_data = pd.read_csv(train_path)\n",
        "        \n",
        "        test_data = pd.read_csv(test_path)\n",
        "        # CONTENT / NAME / CKIPNAME\n",
        "        train_x = [ self.clean_string(i) for i in train_data['full_content'].values.tolist()]\n",
        "        train_y = [ self.name_list(i) for i in train_data['name'].values.tolist()]\n",
        "        train_z = [ self.name_list(i) for i in train_data['ckip_names'].values.tolist() ]\n",
        "\n",
        "        # CONTENT / NAME\n",
        "        test_x = [self.clean_string(i) for i in test_data['full_content'].values.tolist()]\n",
        "        self.test_y = [self.name_list(i) for i in test_data['name'].values.tolist()]\n",
        "        #test_z = [self.name_list(i) for i in test_data['ckip_names'].values.tolist()]\n",
        "\n",
        "        self.train = [] # CONTENT / NAME / (CKIP-NAME)\n",
        "        self.test = []  # CONTENT / NAME\n",
        "\n",
        "        \n",
        "        right,nright = 0,0\n",
        "        for i in range(len(train_x)):\n",
        "          if len(train_y[i]) == 0:\n",
        "            #self.train.append([train_x[i],[],[]])\n",
        "            continue\n",
        "          q = []\n",
        "          a = []\n",
        "          for n in train_y[i]:\n",
        "            right += 1\n",
        "            self.train.append([train_x[i],n,1])\n",
        "\n",
        "          for n in train_z[i]-train_y[i]:\n",
        "            if len(n)==1: # remove the ckip name len is 1\n",
        "              continue\n",
        "            nright += 1\n",
        "            self.train.append([train_x[i],n,0])              \n",
        "\n",
        "        for i in range(len(test_x)):\n",
        "          self.test.append([test_x[i],self.test_y[i],])\n",
        "\n",
        "\n",
        "        print(f'Train: {len(self.train)} Test: {len(self.test)} R/N: {right},{nright}')\n",
        "\n",
        "    def name_list(self,name_list_str):\n",
        "        ls = ast.literal_eval(name_list_str)\n",
        "        res = []\n",
        "        cc = OpenCC('t2s')\n",
        "        for n in ls:\n",
        "          res.append(n)\n",
        "        return set(res)        \n",
        "    def clean_string(self,content):\n",
        "      content = content.replace('\\n','。').replace('\\t','，').replace('!', '！').replace('?', '？').replace('；','。')# erease white space cause English name error\n",
        "      content = re.sub(\"[+\\.\\/_,$%●▼►^*(+\\\"\\']+|[+——~@#￥%……&*（）★]\", \"\",content)\n",
        "      content = re.sub(r\"[%s]+\" %stops, \"。\",content)\n",
        "      content = content.replace(' ','')\n",
        "      #content = self.cc.convert(content)\n",
        "      return content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OGS78UM3NW9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class TrainDataset(Dataset):\n",
        "    def __init__(self,data,model_type):\n",
        "      self.data = data\n",
        "      self.tokenizer = BertTokenizer.from_pretrained(model_type)\n",
        "        \n",
        "    def __getitem__(self,idx):\n",
        "      question , paragraph ,label = self.data[idx][1] , self.data[idx][0],self.data[idx][2]\n",
        "      token_tensor = self.tokenizer.encode_plus(question,paragraph,max_length=512,truncation=True,pad_to_max_length=True)\n",
        "      label_tensor = torch.Tensor([label])\n",
        "      # token / segment / mask / label\n",
        "      return  torch.tensor(token_tensor['input_ids']), torch.tensor( token_tensor['token_type_ids']) , torch.tensor( token_tensor['attention_mask'] ),label_tensor\n",
        "    def __len__(self):\n",
        "      return len(self.data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nX2TV4G7aMb0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class bertwwmQA(nn.Module):\n",
        "    def __init__(self,model_name,config):\n",
        "        super(bertwwmQA,self).__init__()\n",
        "        #self.bert_model = BertModel.from_pretrained(\"hfl/chinese-roberta-wwm-ext-large\")\n",
        "        \n",
        "        self.bert_model = BertModel.from_pretrained(model_name,config = config)\n",
        "       # self.bert_model = BertForQuestionAnswering.from_pretrained(model_name,config = config)\n",
        "        #self.bert_model = BertForSequenceClassification.from_pretrained(model_name,config = config)\n",
        "        self.bi_decoder = nn.Sequential(\n",
        "            nn.Linear(config.hidden_size,config.hidden_size)\n",
        "            ,nn.Dropout(0.1)\n",
        "            ,nn.ReLU()\n",
        "            ,nn.Linear(config.hidden_size,2)\n",
        "        )\n",
        "        self.start()\n",
        "\n",
        "    def start(self):\n",
        "      nn.init.xavier_uniform_(self.bi_decoder[0].weight)\n",
        "      nn.init.constant_(self.bi_decoder[0].bias, 0)\n",
        "      nn.init.xavier_uniform_(self.bi_decoder[3].weight)\n",
        "      nn.init.constant_(self.bi_decoder[3].bias, 0)\n",
        "\n",
        "    def forward(self,input_ids=None,attention_mask=None,token_type_ids=None):\n",
        "       # print(f'receive input_ids {input_ids}')\n",
        "\n",
        "        cls = self.bert_model(input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids)[1]\n",
        "        binary = self.bi_decoder(cls)\n",
        "        return binary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vsuMCh72asM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "x1 = pd.read_csv('./datasetALL/dataset1/tbrain_train.csv')\n",
        "x2 = pd.read_csv('./datasetALL/dataset0/dataset0803.csv')\n",
        "f = x1['full_content'].values.tolist()+x2['full_content'].values.tolist()\n",
        "ck = x1['ckip_names'].values.tolist()+x2['ckip_names'].values.tolist()\n",
        "na = x1['name'].values.tolist()+x2['name'].values.tolist()\n",
        "import csv\n",
        "with open('./datasetALL/dataset1/tbrain_train_ex.csv','w',encoding=\"utf-8\",newline='') as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  writer.writerow(['full_content','ckip_names','name'])\n",
        "  for i in range(len(f)):\n",
        "    writer.writerow([f[i],ck[i],na[i]])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vy5eg-7q49dK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBzwfS1t3qvO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IG5nVEQ9ArVy",
        "colab_type": "text"
      },
      "source": [
        "# 打包啦"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tKTLDDNySx-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "9cb225f2-0f74-461a-8d04-e36531bc5a6e"
      },
      "source": [
        "\"\"\"\n",
        "class QAFilter():\n",
        "  def __init__(self,model_path): \n",
        "    \n",
        "    # device\n",
        "    self.device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    \n",
        "    # model & tokenizer\n",
        "    model_type = 'hfl/chinese-roberta-wwm-ext'\n",
        "    config = BertConfig.from_pretrained(model_type,output_hidden_states=True)\n",
        "    self.tokenizer = BertTokenizer.from_pretrained(model_type)\n",
        "    self.model = bertwwmQA(model_type,config).to(self.device)\n",
        "    self.model.load_state_dict(torch.load(model_path)) \n",
        "  \n",
        "    # 繁簡轉換\n",
        "    self.c2tw = OpenCC('s2t') # china to tw\n",
        "    self.tw2c = OpenCC('t2s') # tw to china\n",
        "\n",
        "  def filter(self,content,name_list):\n",
        "    # name_list : a list like: ['蔡英文','陳水扁']\n",
        "    # content  : a new \"繁體新聞\"\n",
        "    # 會回傳像是 [1,0,0,1] 代表第0和3的人名是對的\n",
        "    content = self.tw2c.convert(content)\n",
        "    pred_nlist = []\n",
        "    with torch.no_grad():\n",
        "      for n in name_list:\n",
        "        n = self.tw2c.convert(n)\n",
        "        token_tensor = self.tokenizer.encode_plus(str(n),str(content),max_length=512,truncation=True,pad_to_max_length=True)\n",
        "        token = torch.tensor(token_tensor['input_ids']).unsqueeze(0).to(self.device)\n",
        "        segment = torch.tensor( token_tensor['token_type_ids']).unsqueeze(0).to(self.device)\n",
        "        mask = torch.tensor( token_tensor['attention_mask'] ).unsqueeze(0).to(self.device)\n",
        "        pred = self.model(input_ids=token,attention_mask=mask,token_type_ids=segment).argmax(dim=1)\n",
        "        #print(n,pred.item())\n",
        "        pred_nlist.append(pred.item())\n",
        "    return pred_nlist\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nclass QAFilter():\\n  def __init__(self,model_path): \\n    \\n    # device\\n    self.device = torch.device(\\'cuda:0\\') if torch.cuda.is_available() else torch.device(\\'cpu\\')\\n    \\n    # model & tokenizer\\n    model_type = \\'hfl/chinese-roberta-wwm-ext\\'\\n    config = BertConfig.from_pretrained(model_type,output_hidden_states=True)\\n    self.tokenizer = BertTokenizer.from_pretrained(model_type)\\n    self.model = bertwwmQA(model_type,config).to(self.device)\\n    self.model.load_state_dict(torch.load(model_path)) \\n  \\n    # 繁簡轉換\\n    self.c2tw = OpenCC(\\'s2t\\') # china to tw\\n    self.tw2c = OpenCC(\\'t2s\\') # tw to china\\n\\n  def filter(self,content,name_list):\\n    # name_list : a list like: [\\'蔡英文\\',\\'陳水扁\\']\\n    # content  : a new \"繁體新聞\"\\n    # 會回傳像是 [1,0,0,1] 代表第0和3的人名是對的\\n    content = self.tw2c.convert(content)\\n    pred_nlist = []\\n    with torch.no_grad():\\n      for n in name_list:\\n        n = self.tw2c.convert(n)\\n        token_tensor = self.tokenizer.encode_plus(str(n),str(content),max_length=512,truncation=True,pad_to_max_length=True)\\n        token = torch.tensor(token_tensor[\\'input_ids\\']).unsqueeze(0).to(self.device)\\n        segment = torch.tensor( token_tensor[\\'token_type_ids\\']).unsqueeze(0).to(self.device)\\n        mask = torch.tensor( token_tensor[\\'attention_mask\\'] ).unsqueeze(0).to(self.device)\\n        pred = self.model(input_ids=token,attention_mask=mask,token_type_ids=segment).argmax(dim=1)\\n        #print(n,pred.item())\\n        pred_nlist.append(pred.item())\\n    return pred_nlist\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJm27rZIL9V5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class QAFilter():\n",
        "  def __init__(self,model_path): \n",
        "    \n",
        "    # device\n",
        "    self.device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    \n",
        "    # model & tokenizer\n",
        "    model_type = 'hfl/chinese-roberta-wwm-ext'\n",
        "    config = BertConfig.from_pretrained(model_type,output_hidden_states=True)\n",
        "    self.tokenizer = BertTokenizer.from_pretrained(model_type)\n",
        "    self.model = bertwwmQA(model_type,config).to(self.device)\n",
        "    self.model.load_state_dict(torch.load(model_path)) \n",
        "  \n",
        "    # 繁簡轉換\n",
        "    self.c2tw = OpenCC('s2t') # china to tw\n",
        "    self.tw2c = OpenCC('t2s') # tw to china\n",
        "\n",
        "  def filter(self,content,name_list):\n",
        "    # name_list : a list like: ['蔡英文','陳水扁']\n",
        "    # content  : a new \"繁體新聞\"\n",
        "    # 會回傳像是 [1,0,0,1] 代表第0和3的人名是對的\n",
        "    content = self.tw2c.convert(content)\n",
        "    pred_nlist = []\n",
        "    with torch.no_grad():\n",
        "      for n in name_list:\n",
        "        n = self.tw2c.convert(n)\n",
        "        token_tensor = self.tokenizer.encode_plus(str(n),str(content),max_length=512,truncation=True,pad_to_max_length=True)\n",
        "        token = torch.tensor(token_tensor['input_ids']).unsqueeze(0).to(self.device)\n",
        "        segment = torch.tensor( token_tensor['token_type_ids']).unsqueeze(0).to(self.device)\n",
        "        mask = torch.tensor( token_tensor['attention_mask'] ).unsqueeze(0).to(self.device)\n",
        "        pred = self.model(input_ids=token,attention_mask=mask,token_type_ids=segment).argmax(dim=1)\n",
        "        #print(n,pred.item())\n",
        "        pred_nlist.append(pred.item())\n",
        "    return pred_nlist\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeU_ooqwpN8T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "89e5701f-4b7e-4f02-f8e5-d56cebaee5ae"
      },
      "source": [
        "\"\"\"\n",
        "def test(model,data,device,ws,pos,ner,model_name):\n",
        "    #b_modelname = 'bertWWM_BI_16_27_53.pt'\n",
        "    #bin_model = bertwwm().to(device)\n",
        "    #bin_model.load_state_dict(torch.load(f'saved_models/{b_modelname}'))\n",
        "    print('Testing...')\n",
        "    test_num = len(data.test)\n",
        "    #criterion = nn.BCELoss()\n",
        "    #b_tokenizer = BertTokenizer.from_pretrained(\"hfl/chinese-bert-wwm\")\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "    model.eval()\n",
        "    print(f'Testing data: {test_num}')\n",
        "    c_to_tw = OpenCC('s2t') # chi to tw\n",
        "    tw_to_c = OpenCC('t2s')\n",
        "    label_count = 0\n",
        "    all_predlist = []\n",
        "    all_labellist = []\n",
        "   # all_labellist = data.test_y\n",
        "    with torch.no_grad():\n",
        "      for content,nlabel in data.test:\n",
        "        if len(nlabel) == 0:\n",
        "         # all_labellist.append([])\n",
        "          continue     \n",
        "       # print(f'test name:')\n",
        "        \n",
        "        if len(content)>512:\n",
        "          content = content[:512] \n",
        "        label_count += 1\n",
        "        ckip_nlist = []\n",
        "        pred_nlist = []\n",
        "        ws_results = ws([c_to_tw.convert(content)])\n",
        "        pos_results = pos(ws_results)\n",
        "        ner_results = ner(ws_results, pos_results)\n",
        "        for name in ner_results[0]:\n",
        "          if name[2] == 'PERSON':\n",
        "            ckip_nlist.append(tw_to_c.convert(name[3]))\n",
        "        ckip_nlist = set(ckip_nlist)\n",
        "        for ckip_name in ckip_nlist:\n",
        "          if len(ckip_name)<2:\n",
        "            continue\n",
        "          token_tensor = tokenizer.encode_plus(str(ckip_name),str(content),max_length=512,truncation=True,pad_to_max_length=True)\n",
        "          token = torch.tensor(token_tensor['input_ids']).unsqueeze(0).to(device)\n",
        "          segment = torch.tensor( token_tensor['token_type_ids']).unsqueeze(0).to(device)\n",
        "          mask = torch.tensor( token_tensor['attention_mask'] ).unsqueeze(0).to(device)\n",
        "          pred = model(input_ids=token,attention_mask=mask,token_type_ids=segment).argmax(dim=1)\n",
        "          if pred.item() == 1:\n",
        "            pred_nlist.append(ckip_name)\n",
        "       # pred_nlist = sorted(pred_nlist, key = lambda s: s[1])\n",
        "        all_labellist.append(nlabel)\n",
        "        all_predlist.append(pred_nlist)\n",
        "        print(f'CKIP:{ckip_nlist} PRED:{pred_nlist} LABEL:{nlabel}')\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "    return all_predlist,all_labellist\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ndef test(model,data,device,ws,pos,ner,model_name):\\n    #b_modelname = \\'bertWWM_BI_16_27_53.pt\\'\\n    #bin_model = bertwwm().to(device)\\n    #bin_model.load_state_dict(torch.load(f\\'saved_models/{b_modelname}\\'))\\n    print(\\'Testing...\\')\\n    test_num = len(data.test)\\n    #criterion = nn.BCELoss()\\n    #b_tokenizer = BertTokenizer.from_pretrained(\"hfl/chinese-bert-wwm\")\\n    tokenizer = BertTokenizer.from_pretrained(model_name)\\n    model.eval()\\n    print(f\\'Testing data: {test_num}\\')\\n    c_to_tw = OpenCC(\\'s2t\\') # chi to tw\\n    tw_to_c = OpenCC(\\'t2s\\')\\n    label_count = 0\\n    all_predlist = []\\n    all_labellist = []\\n   # all_labellist = data.test_y\\n    with torch.no_grad():\\n      for content,nlabel in data.test:\\n        if len(nlabel) == 0:\\n         # all_labellist.append([])\\n          continue     \\n       # print(f\\'test name:\\')\\n        \\n        if len(content)>512:\\n          content = content[:512] \\n        label_count += 1\\n        ckip_nlist = []\\n        pred_nlist = []\\n        ws_results = ws([c_to_tw.convert(content)])\\n        pos_results = pos(ws_results)\\n        ner_results = ner(ws_results, pos_results)\\n        for name in ner_results[0]:\\n          if name[2] == \\'PERSON\\':\\n            ckip_nlist.append(tw_to_c.convert(name[3]))\\n        ckip_nlist = set(ckip_nlist)\\n        for ckip_name in ckip_nlist:\\n          if len(ckip_name)<2:\\n            continue\\n          token_tensor = tokenizer.encode_plus(str(ckip_name),str(content),max_length=512,truncation=True,pad_to_max_length=True)\\n          token = torch.tensor(token_tensor[\\'input_ids\\']).unsqueeze(0).to(device)\\n          segment = torch.tensor( token_tensor[\\'token_type_ids\\']).unsqueeze(0).to(device)\\n          mask = torch.tensor( token_tensor[\\'attention_mask\\'] ).unsqueeze(0).to(device)\\n          pred = model(input_ids=token,attention_mask=mask,token_type_ids=segment).argmax(dim=1)\\n          if pred.item() == 1:\\n            pred_nlist.append(ckip_name)\\n       # pred_nlist = sorted(pred_nlist, key = lambda s: s[1])\\n        all_labellist.append(nlabel)\\n        all_predlist.append(pred_nlist)\\n        print(f\\'CKIP:{ckip_nlist} PRED:{pred_nlist} LABEL:{nlabel}\\')\\n        \\n\\n\\n\\n    return all_predlist,all_labellist\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX9jxOcdJk8N",
        "colab_type": "text"
      },
      "source": [
        "## 繁體測試"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rMCJ5RvJf4J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model,data,device,ws,pos,ner,model_name):\n",
        "    print('Testing...')\n",
        "    test_num = len(data.test)\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "    model.eval()\n",
        "    print(f'Testing data: {test_num}')\n",
        "    label_count = 0\n",
        "    all_predlist = []\n",
        "    all_labellist = []\n",
        "    with torch.no_grad():\n",
        "      for content,nlabel in data.test:\n",
        "        if len(nlabel) == 0:\n",
        "          continue           \n",
        "        #if len(content)>512:\n",
        "        #  content = content[:512] \n",
        "        label_count += 1\n",
        "        ckip_nlist = []\n",
        "        pred_nlist = []\n",
        "        ws_results = ws([content])\n",
        "        pos_results = pos(ws_results)\n",
        "        ner_results = ner(ws_results, pos_results)\n",
        "        for name in ner_results[0]:\n",
        "          if name[2] == 'PERSON':\n",
        "            ckip_nlist.append(name[3])\n",
        "        ckip_nlist = set(ckip_nlist)\n",
        "        for ckip_name in ckip_nlist:\n",
        "          if len(ckip_name)<2:\n",
        "            continue\n",
        "          token_tensor = tokenizer.encode_plus(str(ckip_name),str(content),max_length=512,truncation=True,pad_to_max_length=True)\n",
        "          token = torch.tensor(token_tensor['input_ids']).unsqueeze(0).to(device)\n",
        "          segment = torch.tensor( token_tensor['token_type_ids']).unsqueeze(0).to(device)\n",
        "          mask = torch.tensor( token_tensor['attention_mask'] ).unsqueeze(0).to(device)\n",
        "          pred = model(input_ids=token,attention_mask=mask,token_type_ids=segment).argmax(dim=1)\n",
        "          if pred.item() == 1:\n",
        "            pred_nlist.append(ckip_name)\n",
        "       # pred_nlist = sorted(pred_nlist, key = lambda s: s[1])\n",
        "        all_labellist.append(nlabel)\n",
        "        all_predlist.append(pred_nlist)\n",
        "        print(f'CKIP:{ckip_nlist} PRED:{pred_nlist} LABEL:{nlabel}')\n",
        "\n",
        "    return all_predlist,all_labellist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtksFQ2ghOFB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def testBIN(model,data,device,ws,pos,ner,model_name):\n",
        "    print('Testing...')\n",
        "    acc = 0\n",
        "    total = len(data.test)\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "    with torch.no_grad():\n",
        "      for content,name in data.test:\n",
        "        input_ids = torch.tensor([tokenizer.encode(str(content),max_length=512,truncation=True,pad_to_max_length=True)]).to(device)   \n",
        "        b_pred = model(input_ids).to(device) \n",
        "        if b_pred.item() < 0.4 and len(name) == 0:\n",
        "          acc +=1\n",
        "        elif b_pred.item() > 0.4 and len(name):\n",
        "          acc +=1\n",
        "        print(f'C: {content[:20]} Name: {name} Pred: {b_pred.item()}')\n",
        "    print(f'ALL {total} ACC {acc}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrFxvpVYYQe2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(trainLoader,w_d,lr_rate,device,model_type,epoches):\n",
        "  config = BertConfig.from_pretrained(model_type, output_hidden_states=True)\n",
        "  model = bertwwmQA(model_type,config).to(device)\n",
        "  parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "  optimizer = AdamW(parameters, lr=lr_rate, weight_decay=w_d)\n",
        "  #criterion = nn.BCELoss()\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  model.train()\n",
        "  minloss = 100000\n",
        "  for e in range(epoches):\n",
        "    loss,i = 0,0\n",
        "    for data in trainLoader:\n",
        "      i+=1\n",
        "      token , segment , mask , label = [ item.to(device) for item in data]\n",
        "      pred = torch.softmax(model(input_ids=token,attention_mask=mask,token_type_ids=segment),dim=-1)   \n",
        "      label = label.reshape(label.shape[0])\n",
        "     # print(pred.shape,label.shape)\n",
        "      batch_loss = criterion(pred,label.long())\n",
        "      loss += batch_loss\n",
        "     # print(batch_loss)\n",
        "      batch_loss.backward()\n",
        "     # print(f'Epoch {e} Batch {i} Loss is {batch_loss}')\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "      #print(f'Loss {batch_loss}')\n",
        "    print(f'Epoches: {e} Loss {loss} AVG: {loss/i}')\n",
        "\n",
        "    if loss < minloss:\n",
        "      minloss = loss\n",
        "      best_model = copy.deepcopy(model.state_dict())\n",
        "  return best_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYwVSH-TvsBu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model_type = 'hfl/chinese-bert-wwm'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5HADkK4DefY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#mydata = mydata_nosep('./data0/tbrain_train.csv','./data0/tbrain_test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFRr64FirS8O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fc39c5dd-fd16-466c-e3d4-86167023ecc9"
      },
      "source": [
        "\n",
        "!pip install -U ckiptagger[tfgpu,gdown]\n",
        "from ckiptagger import WS, POS, NER\n",
        "ws = WS(\"./data\")\n",
        "pos = POS(\"./data\")\n",
        "ner = NER(\"./data\")\n",
        "print('CKIP finish')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ckiptagger[gdown,tfgpu]\n",
            "  Downloading https://files.pythonhosted.org/packages/5d/24/9ee7289b423345bc1705453437d9b0d9e93a015fbc00885c6033c2f50fab/ckiptagger-0.1.1-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: gdown; extra == \"gdown\" in /usr/local/lib/python3.6/dist-packages (from ckiptagger[gdown,tfgpu]) (3.6.4)\n",
            "Collecting tensorflow-gpu<2,>=1.13.1; extra == \"tfgpu\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/ab/19aba3629427c2d96790f73838639136ce02b6e7e1c4f2dd60149174c794/tensorflow_gpu-1.15.3-cp36-cp36m-manylinux2010_x86_64.whl (411.0MB)\n",
            "\u001b[K     |████████████████████████████████| 411.0MB 39kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from gdown; extra == \"gdown\"->ckiptagger[gdown,tfgpu]) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from gdown; extra == \"gdown\"->ckiptagger[gdown,tfgpu]) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from gdown; extra == \"gdown\"->ckiptagger[gdown,tfgpu]) (1.15.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 43.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2,>=1.13.1; extra == \"tfgpu\"->ckiptagger[gdown,tfgpu]) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2,>=1.13.1; extra == \"tfgpu\"->ckiptagger[gdown,tfgpu]) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2,>=1.13.1; extra == \"tfgpu\"->ckiptagger[gdown,tfgpu]) (0.34.2)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2,>=1.13.1; extra == \"tfgpu\"->ckiptagger[gdown,tfgpu]) (0.2.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2,>=1.13.1; extra == \"tfgpu\"->ckiptagger[gdown,tfgpu]) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2,>=1.13.1; extra == \"tfgpu\"->ckiptagger[gdown,tfgpu]) (3.12.4)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2,>=1.13.1; extra == \"tfgpu\"->ckiptagger[gdown,tfgpu]) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2,>=1.13.1; extra == \"tfgpu\"->ckiptagger[gdown,tfgpu]) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2,>=1.13.1; extra == \"tfgpu\"->ckiptagger[gdown,tfgpu]) (3.3.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 48.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2,>=1.13.1; extra == \"tfgpu\"->ckiptagger[gdown,tfgpu]) (1.30.0)\n",
            "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2,>=1.13.1; extra == \"tfgpu\"->ckiptagger[gdown,tfgpu]) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gdown; extra == \"gdown\"->ckiptagger[gdown,tfgpu]) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gdown; extra == \"gdown\"->ckiptagger[gdown,tfgpu]) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gdown; extra == \"gdown\"->ckiptagger[gdown,tfgpu]) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gdown; extra == \"gdown\"->ckiptagger[gdown,tfgpu]) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu<2,>=1.13.1; extra == \"tfgpu\"->ckiptagger[gdown,tfgpu]) (49.2.0)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu<2,>=1.13.1; extra == \"tfgpu\"->ckiptagger[gdown,tfgpu]) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu<2,>=1.13.1; extra == \"tfgpu\"->ckiptagger[gdown,tfgpu]) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu<2,>=1.13.1; extra == \"tfgpu\"->ckiptagger[gdown,tfgpu]) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu<2,>=1.13.1; extra == \"tfgpu\"->ckiptagger[gdown,tfgpu]) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu<2,>=1.13.1; extra == \"tfgpu\"->ckiptagger[gdown,tfgpu]) (3.1.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=a61e9556738849714d86262b0527a517386cf16e2ef0efd6f16ecb0a9064cee4\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement tensorboard<3,>=2.3.0, but you'll have tensorboard 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement tensorflow-estimator<2.4.0,>=2.3.0, but you'll have tensorflow-estimator 1.15.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, gast, keras-applications, tensorboard, tensorflow-gpu, ckiptagger\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "Successfully installed ckiptagger-0.1.1 gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gast",
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-caaf86bc141d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install -U ckiptagger[tfgpu,gdown]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mckiptagger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPOS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mws\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPOS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNER\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ckiptagger/api.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_dir, disable_cuda)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_ws\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ckiptagger/model_ws.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_hyper_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ckiptagger/model_ws.py\u001b[0m in \u001b[0;36mcreate_embedding\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;34m\"\"\"Create a trainable unknown vector.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_normal_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstddev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munknown_w_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unknown_w_v\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_embedding_d\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_feature_d\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'variable_scope'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNblMv7KKhH9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
        "for i in range(1,11):\n",
        "  print(f'Train QA in dataset {i}')\n",
        "  tpath = './datasetALL/dataset'+ str(i) + '/tbrain_train.csv'\n",
        "  print(tpath)\n",
        "  mydata = mydata_nosep(tpath,'./data0/tbrain_test.csv')\n",
        "  model_type = 'hfl/chinese-roberta-wwm-ext'\n",
        "  train_ds = TrainDataset(mydata.train,model_type)\n",
        "  trainLoader = DataLoader(train_ds, batch_size=8)\n",
        "  config = BertConfig.from_pretrained(model_type,output_hidden_states=True)\n",
        "  best_model = train(trainLoader,0.001,1e-5,device,model_type,10)\n",
        "  if not os.path.exists('saved_models'):\n",
        "    os.makedirs('saved_models')    \n",
        "  \n",
        "  modelname = 'bertWWM_QA'+ str(i) + '_'\n",
        "  torch.save(best_model, f'saved_models/{modelname}.pt')\n",
        "  print(f'Train end, model name is {modelname}.pt')\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8irEwsitz1_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkBuVhyvKTnQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mydata = mydata_nosep('./datasetALL/dataset1/tbrain_train_ex.csv','./datasetALL/dataset1/tbrain_test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AaEG_2o-LOV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mydata.train[:10]\n",
        "mydata.test[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xfxWQASyQFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "model_type = 'hfl/chinese-roberta-wwm-ext'\n",
        "torch.cuda.empty_cache()  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8p5k8zOJ5xL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mode = 'testqa'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1F3jttAwYQom",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache() \n",
        "if mode == 'trainqa':\n",
        "  model_type = \"bert-base-chinese\"\n",
        "  #model_type = 'hfl/chinese-bert-wwm'\n",
        "  #model_type = 'hfl/chinese-roberta-wwm-ext-large'\n",
        "  train_ds = TrainDataset(mydata.train,model_type)\n",
        "  trainLoader = DataLoader(train_ds, batch_size=8)\n",
        "  config = BertConfig.from_pretrained(model_type,output_hidden_states=True)\n",
        "  print('Train QA')\n",
        "  best_model = train(trainLoader,0.001,1e-5,device,model_type,10)\n",
        "  if not os.path.exists('saved_models'):\n",
        "    os.makedirs('saved_models')    \n",
        "  modeltime = strftime('%H_%M_%S', gmtime()) \n",
        "  modelname = 'bertWWM_QA_'+ modeltime\n",
        "  torch.save(best_model, f'saved_models/{modelname}.pt')\n",
        "  print(f'Train end, model name is {modelname}.pt')\n",
        "\n",
        "elif mode == 'testqa':\n",
        "  print('Testqa')\n",
        "  #model_type = 'hfl/chinese-roberta-wwm-ext'\n",
        "  model_type = \"bert-base-chinese\"\n",
        "  modelname = 'bertWWM_QA_09_44_57.pt'\n",
        "  config = BertConfig.from_pretrained(model_type,output_hidden_states=True)\n",
        "\n",
        "  test_model = bertwwmQA(model_type,config).to(device)\n",
        "  test_model.load_state_dict(torch.load(f'saved_models/{modelname}'))\n",
        "  all_predlist,all_labellist = test(test_model,mydata,device,ws,pos,ner,model_type)\n",
        "  print(f'Testend point is {eval_all(all_predlist,all_labellist)}')\n",
        "# train binary classfier\n",
        "elif mode == 'trainbin':\n",
        "  model_type = 'hfl/chinese-bert-wwm'\n",
        "  best_model = trainBinary(data,0.005,5e-4,device,model_type)\n",
        "  if not os.path.exists('saved_models'):\n",
        "    os.makedirs('saved_models')    \n",
        "  modeltime = strftime('%H_%M_%S', gmtime()) \n",
        "  modelname = 'bertWWM_BIN_'+ modeltime\n",
        "  torch.save(best_model, f'saved_models/{modelname}.pt')\n",
        "  print(f'Train end, model name is {modelname}.pt')\n",
        "\n",
        "elif mode == 'testbin':\n",
        "  modelname = 'bertWWM_BIN_15_05_01.pt'\n",
        "  model_type = 'hfl/chinese-bert-wwm'\n",
        " # test_model = bertwwm(model_type).to(device)\n",
        "  test_model = bertwwm(model_type).to(device)\n",
        "  test_model.load_state_dict(torch.load(f'saved_models/{modelname}'))\n",
        "  \n",
        "  testBIN(test_model,data,device,ws,pos,ner,model_type)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTiGi7AfvTNu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvsXGuO3klay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pVyMilGLa_E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f'Testend point is {eval_all(all_predlist,all_labellist)}')\n",
        "\n",
        "# bert-base-chinese 1e-5 : 0.88 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U1DNwvvirw6B",
        "colab": {}
      },
      "source": [
        "print(f'Testend point is {eval_all(all_predlist,all_labellist)}')\n",
        "\n",
        "# bert-base-chinese 1e-5 : 0.88 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O25agBgLg62Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#RoBERTa-wwm-ext-large\thfl/chinese-roberta-wwm-ext-large\n",
        "#RoBERTa-wwm-ext\thfl/chinese-roberta-wwm-ext\n",
        "#BERT-wwm-ext\thfl/chinese-bert-wwm-ext\n",
        "#BERT-wwm\thfl/chinese-bert-wwm\n",
        "#RBT3\thfl/rbt3\n",
        "#RBTL3\thfl/rbtl3"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}